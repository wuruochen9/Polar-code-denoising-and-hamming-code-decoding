{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "buxianggaile.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "!pip install py-polar-codes"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting py-polar-codes\n",
            "  Downloading py-polar-codes-1.2.2.tar.gz (18 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from py-polar-codes) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from py-polar-codes) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->py-polar-codes) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->py-polar-codes) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->py-polar-codes) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->py-polar-codes) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->py-polar-codes) (1.15.0)\n",
            "Building wheels for collected packages: py-polar-codes\n",
            "  Building wheel for py-polar-codes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-polar-codes: filename=py_polar_codes-1.2.2-py3-none-any.whl size=22941 sha256=249533bbe15c15971e82e2794faed3804e3dc154236129720e9e2f6536079c45\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/de/72/c2376d6da5c139b52ca3312e54c512a44a6cf1ebd3069e0008\n",
            "Successfully built py-polar-codes\n",
            "Installing collected packages: py-polar-codes\n",
            "Successfully installed py-polar-codes-1.2.2\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkdoRMpyrLXT",
        "outputId": "c7cca664-2d8c-4b88-910b-ee0dba19b077"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "!pip install wget"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9673 sha256=6885506fe20c92ff9e4fc6bc95cbd781fa01a888de83132fb2a0615afc11a06f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trVafcPlBfto",
        "outputId": "a331ee49-fefe-40fb-d4c6-7997119d7f38"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "import tensorflow.keras.backend as K\r\n",
        "from polarcodes import *\r\n",
        "import pandas as pd\r\n",
        "import math\r\n",
        "import os"
      ],
      "outputs": [],
      "metadata": {
        "id": "BBO9Yf7qreKj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzt1V8oDCvmF",
        "outputId": "5b3584bf-ffe4-4c78-d14d-81acc59e5723"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "path = \"/content/drive/My Drive/Colab Notebooks/\"\r\n",
        "os.chdir(path)\r\n",
        "os.listdir(path)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['“On Deep Learning-Based Channel Decoding.ipynb”的副本',\n",
              " 'Untitled',\n",
              " 'doudizhu.ipynb',\n",
              " 'bert_tune.ipynb',\n",
              " 'attention-is-all-you-need-pytorch-master',\n",
              " 'BERT4doc-Classification-master',\n",
              " 'Untitled0.ipynb',\n",
              " 'On Deep Learning-Based Channel Decoding-checkpoint.ipynb',\n",
              " '“Transformer-Torch”的副本',\n",
              " '“Transformer_Torch”的副本.ipynb',\n",
              " 'paper_implementation (2).ipynb',\n",
              " 'BERT-Based Channel Decoding.ipynb',\n",
              " 'para.pth',\n",
              " 'BERT_Based_Channel_Decoding (1).ipynb',\n",
              " 'loss8.txt',\n",
              " 'loss9.txt',\n",
              " 'test3(final).ipynb',\n",
              " 'FINAL (2).ipynb',\n",
              " 'FINAL (1).ipynb',\n",
              " 'code.csv',\n",
              " 'para2.pth',\n",
              " 'FINAL.ipynb',\n",
              " 'paper_implementation (1).ipynb',\n",
              " 'paper_implementation.ipynb',\n",
              " 'hammingcode.csv',\n",
              " 'buxianggaile.ipynb']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AKidz8KDUhJ",
        "outputId": "90d63a26-84a7-4a8a-a2a4-502245991c29"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#generate haiming code \r\n",
        "class hamming_code_generator():\r\n",
        "    def __init___(self):\r\n",
        "        pass\r\n",
        "    def pint2bin(self,num):\r\n",
        "        num=num%(int(2**(self.length-1)))\r\n",
        "        bin_str=''\r\n",
        "        while num!=0:\r\n",
        "            res=num%2\r\n",
        "            num=math.floor(num/2)\r\n",
        "            bin_str=str(res)+bin_str\r\n",
        "        bin_str='0'*(self.length-len(bin_str))+bin_str\r\n",
        "        return bin_str\r\n",
        "\r\n",
        "\r\n",
        "    def get_manage_dict(self):\r\n",
        "        parity_bits=[int(2**i) for i in range(self.pbn)]  #[1,2,4,8,16....]\r\n",
        "        manage_dict={}\r\n",
        "        for parity_bit in parity_bits:\r\n",
        "            manage_dict[parity_bit]=[i for i in range(1,self.newlength+1) if i&parity_bit!=0]\r\n",
        "        print(manage_dict)\r\n",
        "        return manage_dict\r\n",
        "\r\n",
        "    def init_haiming_code(self,bin_str):\r\n",
        "        bit=1\r\n",
        "        self.pbn=0\r\n",
        "        h_code=''\r\n",
        "        for i in range(self.length):\r\n",
        "            while(math.log(bit,2).is_integer()):\r\n",
        "                h_code='0'+h_code\r\n",
        "                bit+=1\r\n",
        "                self.pbn+=1\r\n",
        "            h_code=bin_str[self.length-1-i]+h_code\r\n",
        "            bit+=1\r\n",
        "        return h_code\r\n",
        "\r\n",
        "    def process(self,nums=100,length=8):\r\n",
        "        self.length=length\r\n",
        "        h_codes=[]\r\n",
        "        for num in range(nums):\r\n",
        "            h_codes.append([int(c) for c in self.init_haiming_code(self.pint2bin(num))])\r\n",
        "        self.newlength=self.length+self.pbn\r\n",
        "        self.manage_dict=self.get_manage_dict()\r\n",
        "        for i in range(len(h_codes)):\r\n",
        "            h_code=h_codes[i]\r\n",
        "            for parity_bit, manage_bits in self.manage_dict.items():\r\n",
        "                odd=sum([h_code[self.newlength-i] for i in manage_bits])%2\r\n",
        "                if not odd:\r\n",
        "                    h_code[self.newlength-parity_bit]=1\r\n",
        "            h_codes[i]=h_code\r\n",
        "            # print(i)\r\n",
        "            # print(h_code)\r\n",
        "        return h_codes\r\n",
        "    # def get_parity_bits_num(self):\r\n",
        "    #     for pbn in range(int(self.length/2)):\r\n",
        "    #         if 2**pbn-1>self.length+pbn or 2**pbn-1==self.length+pbn:\r\n",
        "    #             return pbn\r\n",
        "def generate_input_data(k):#载入训练数据\r\n",
        "    X = []\r\n",
        "    for i in range(2**k):\r\n",
        "        bin_str = bin(i)[2:].zfill(k)\r\n",
        "        x = []\r\n",
        "        for j in range(k):\r\n",
        "            x.append(int(bin_str[j]))\r\n",
        "        X.append(x)\r\n",
        "    return np.array(X)\r\n",
        "\r\n",
        "#8->12\r\n",
        "a=hamming_code_generator()\r\n",
        "#00000000-11111111\r\n",
        "train_codes=a.process(256,8)\r\n",
        "label = np.array(generate_input_data(8), dtype = np.float32)\r\n",
        "print(label)\r\n",
        "print(train_codes)\r\n",
        "print(a.pint2bin(5))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TetjS2T2dlnd",
        "outputId": "b913715f-5ee5-4cf2-ebd6-be668dd2814b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "#将数据导入code.csv\r\n",
        "import csv\r\n",
        "writer=csv.writer(open('./hammingcode.csv','w'))\r\n",
        "length_list=len(train_codes)\r\n",
        "i=0\r\n",
        "while i!=length_list+1 :\r\n",
        "    if(i==0):\r\n",
        "      data='code'\r\n",
        "      data2='encode'\r\n",
        "    else:\r\n",
        "      # print(i)\r\n",
        "      data=label[i-1]\r\n",
        "      data2=train_codes[i-1]\r\n",
        "    #print data\r\n",
        "    i=i+1\r\n",
        "    writer.writerow([data,data2])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# city = pd.DataFrame([u_messages[0],1])\r\n",
        "# city.to_csv('./code.csv')"
      ],
      "outputs": [],
      "metadata": {
        "id": "pBXtDjtqA-oW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "df = pd.read_csv(\"./hammingcode.csv\", delimiter=',', header=None, names=['code','encode'])\r\n",
        "df.sample(10)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          code                                encode\n",
              "1    [0. 0. 0. 0. 0. 0. 0. 0.]  [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1]\n",
              "237  [1. 1. 1. 0. 1. 1. 0. 0.]  [0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1]\n",
              "94   [0. 1. 0. 1. 1. 1. 0. 1.]  [0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
              "150  [1. 0. 0. 1. 0. 1. 0. 1.]  [0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1]\n",
              "152  [1. 0. 0. 1. 0. 1. 1. 1.]  [0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0]\n",
              "78   [0. 1. 0. 0. 1. 1. 0. 1.]  [0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0]\n",
              "232  [1. 1. 1. 0. 0. 1. 1. 1.]  [0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]\n",
              "254  [1. 1. 1. 1. 1. 1. 0. 1.]  [0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1]\n",
              "125  [0. 1. 1. 1. 1. 1. 0. 0.]  [0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0]\n",
              "231  [1. 1. 1. 0. 0. 1. 1. 0.]  [0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code</th>\n",
              "      <th>encode</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0. 0. 0. 0. 0. 0. 0. 0.]</td>\n",
              "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237</th>\n",
              "      <td>[1. 1. 1. 0. 1. 1. 0. 0.]</td>\n",
              "      <td>[0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>[0. 1. 0. 1. 1. 1. 0. 1.]</td>\n",
              "      <td>[0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>[1. 0. 0. 1. 0. 1. 0. 1.]</td>\n",
              "      <td>[0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>[1. 0. 0. 1. 0. 1. 1. 1.]</td>\n",
              "      <td>[0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>[0. 1. 0. 0. 1. 1. 0. 1.]</td>\n",
              "      <td>[0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232</th>\n",
              "      <td>[1. 1. 1. 0. 0. 1. 1. 1.]</td>\n",
              "      <td>[0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>[1. 1. 1. 1. 1. 1. 0. 1.]</td>\n",
              "      <td>[0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>[0. 1. 1. 1. 1. 1. 0. 0.]</td>\n",
              "      <td>[0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>[1. 1. 1. 0. 0. 1. 1. 0.]</td>\n",
              "      <td>[0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "-yTRrJbhL-uk",
        "outputId": "3a83c8e1-4c8b-44fe-8f1a-31e6a3e75bf9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "\r\n",
        "code=df.code.values\r\n",
        "encode=df.encode.values\r\n",
        "# #encode=encode[1:854]\r\n",
        "code=code[1:]\r\n",
        "encode=encode[1:]\r\n",
        "print(code[-1])\r\n",
        "\r\n",
        "print(encode[-1])\r\n",
        "# print(encode[0:10])\r\n",
        "# # xx=df.label.values\r\n",
        "# # print(xx)\r\n",
        "# print(len(encode))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "[0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdEqlIRxgIAl",
        "outputId": "db560774-6f9b-46c9-d79f-22d1f2d4c4b7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import torch"
      ],
      "outputs": [],
      "metadata": {
        "id": "4b0ZcRP9kLdG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "Y5idJh7lfqw-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "##分词，构建输入输出词表 create input output dictionary\r\n",
        "# S: Symbol that shows starting of decoding input\r\n",
        "# E: Symbol that shows starting of decoding output\r\n",
        "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\r\n",
        "import math\r\n",
        "import torch\r\n",
        "import numpy as np\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.utils.data as Data\r\n",
        "import re\r\n",
        "# Padding Should be Zero\r\n",
        "src_vocab = {'P' : 0,'0':1,'1':2}\r\n",
        "\r\n",
        "# for sent in encode:\r\n",
        "#   sent=sent[1:-1]\r\n",
        "#   sent=sent.split()\r\n",
        "#   for s in sent:\r\n",
        "#     if(s in src_vocab.keys()):\r\n",
        "#       continue\r\n",
        "#     src_vocab[s]=i+1\r\n",
        "#     i=i+1\r\n",
        "\r\n",
        "# print(len(encode))\r\n",
        "# for i in range(len(encode)):\r\n",
        "# for j in range(len(encode[0])):\r\n",
        "#   print(encode[i][j])\r\n",
        "#     # if(encode[i][j] in src_vocab.keys()):\r\n",
        "#     #   continue\r\n",
        "#     # src_vocab[encode[i][j]]=i*8+j+1\r\n",
        "print(src_vocab)\r\n",
        "src_vocab_size = len(src_vocab)\r\n",
        "batch_size=32\r\n",
        "tgt_vocab = {'P' : 0, '0.' : 1, '1.' : 2, 'S' : 3, 'E' : 4}\r\n",
        "idx2word = {i: w for i, w in enumerate(tgt_vocab)}\r\n",
        "tgt_vocab_size = len(tgt_vocab)\r\n",
        "\r\n",
        "src_len = 16 # enc_input max sequence length\r\n",
        "tgt_len = 8 # dec_input(=dec_output) max sequence length\r\n",
        "# Transformer Parameters\r\n",
        "d_model = 512  # Embedding Size\r\n",
        "d_ff = 2048 # FeedForward dimension\r\n",
        "d_k = d_v = 64  # dimension of K(=Q), V\r\n",
        "\r\n",
        "n_layers = 6  # number of Encoder of Decoder Layer\r\n",
        "n_heads = 8  # number of heads in Multi-Head Attention"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'P': 0, '0': 1, '1': 2}\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYPPsorUf8pN",
        "outputId": "bfc72ddb-3e04-4063-d88f-c61f6992ac78"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def make_data(encode,code):#数据预处理 translate codes and haiming codes according to dictionary\r\n",
        "    enc_inputs, dec_inputs, dec_outputs = [], [], []\r\n",
        "    for i in range(len(encode)):\r\n",
        "      en=encode[i][1:-1].split(',')\r\n",
        "      al=[]\r\n",
        "      for j in range(len(en)):\r\n",
        "        a=en[j][-1]\r\n",
        "        al.append(a)\r\n",
        "      #print(al)\r\n",
        "      co=code[i][1:-1].split(' ')\r\n",
        "      #print(co)\r\n",
        "      #print(co)\r\n",
        "      enc_input = [[src_vocab[n] for n in al]] # [[1, 2, 3, 4, 0], [1, 2, 3, 5, 0]]\r\n",
        "      dec_input=[3]\r\n",
        "      dec_output=[]\r\n",
        "\r\n",
        "      for n in co:\r\n",
        "        dec_input.append(tgt_vocab[n])\r\n",
        "        dec_output.append(tgt_vocab[n])\r\n",
        "       # [[6, 1, 2, 3, 4, 8], [6, 1, 2, 3, 5, 8]]\r\n",
        "       # [[1, 2, 3, 4, 8, 7], [1, 2, 3, 5, 8, 7]]\r\n",
        "      # print(dec_input)\r\n",
        "      \r\n",
        "      dec_output.append(4)\r\n",
        "      dec_output=[dec_output]\r\n",
        "      dec_input=[dec_input]\r\n",
        "      enc_inputs.extend(enc_input)\r\n",
        "      dec_inputs.extend(dec_input)\r\n",
        "      dec_outputs.extend(dec_output)\r\n",
        "      \r\n",
        "    print(enc_inputs)\r\n",
        "    print(dec_inputs)\r\n",
        "    print(dec_outputs)\r\n",
        "    return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)\r\n",
        "    #\r\n",
        "\r\n",
        "enc_inputs, dec_inputs, dec_outputs = make_data(encode,code)\r\n",
        "# \r\n",
        "class MyDataSet(Data.Dataset):\r\n",
        "  def __init__(self, enc_inputs, dec_inputs, dec_outputs):\r\n",
        "    super(MyDataSet, self).__init__()\r\n",
        "    self.enc_inputs = enc_inputs\r\n",
        "    self.dec_inputs = dec_inputs\r\n",
        "    self.dec_outputs = dec_outputs\r\n",
        "  \r\n",
        "  def __len__(self):\r\n",
        "    return self.enc_inputs.shape[0]\r\n",
        "  \r\n",
        "  def __getitem__(self, idx):\r\n",
        "    return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\r\n",
        "\r\n",
        "a1=enc_inputs\r\n",
        "a2=dec_inputs\r\n",
        "a3=dec_outputs\r\n",
        "# b1=enc_inputs[int(len(enc_inputs)/8*7):]\r\n",
        "# b2=dec_inputs[int(len(dec_inputs)/8*7):]\r\n",
        "# b3=dec_outputs[int(len(dec_outputs)/8*7):]\r\n",
        "training_data=MyDataSet(a1,a2,a3)\r\n",
        "#test_data=MyDataSet(b1,b2,b3)\r\n",
        "train_loader = Data.DataLoader(training_data,batch_size,True)\r\n",
        "#test_loader=Data.DataLoader(test_data,batch_size,False)\r\n",
        "\r\n",
        "#loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), batch_size, True)\r\n",
        "#print(len(encod"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsQWy11vqEiE",
        "outputId": "8104a893-aead-4b03-c6f2-7c17410ee8a6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer"
      ],
      "metadata": {
        "id": "ERzfa0Qvz-16"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "source": [
        "class PositionalEncoding(nn.Module):\r\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\r\n",
        "        super(PositionalEncoding, self).__init__()\r\n",
        "        self.dropout = nn.Dropout(p=dropout)\r\n",
        "\r\n",
        "        pe = torch.zeros(max_len, d_model)\r\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\r\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\r\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\r\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\r\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\r\n",
        "        self.register_buffer('pe', pe)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        '''\r\n",
        "        x: [seq_len, batch_size, d_model]\r\n",
        "        '''\r\n",
        "        x = x + self.pe[:x.size(0), :]\r\n",
        "        return self.dropout(x)\r\n",
        "\r\n",
        "def get_attn_pad_mask(seq_q, seq_k):\r\n",
        "    '''\r\n",
        "    seq_q: [batch_size, seq_len]\r\n",
        "    seq_k: [batch_size, seq_len]\r\n",
        "    seq_len could be src_len or it could be tgt_len\r\n",
        "    seq_len in seq_q and seq_len in seq_k maybe not equal\r\n",
        "    '''\r\n",
        "    batch_size, len_q = seq_q.size()\r\n",
        "    batch_size, len_k = seq_k.size()\r\n",
        "    # eq(zero) is PAD token\r\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # [batch_size, 1, len_k], False is masked\r\n",
        "    #print(pad_attn_mask)\r\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # [batch_size, len_q, len_k]\r\n",
        "\r\n",
        "def get_attn_subsequence_mask(seq):\r\n",
        "    '''\r\n",
        "    seq: [batch_size, tgt_len]\r\n",
        "    '''\r\n",
        "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\r\n",
        "    subsequence_mask = np.triu(np.ones(attn_shape), k=1) # Upper triangular matrix\r\n",
        "    subsequence_mask = torch.from_numpy(subsequence_mask).byte()\r\n",
        "    return subsequence_mask # [batch_size, tgt_len, tgt_len]"
      ],
      "outputs": [],
      "metadata": {
        "id": "dlWIEHkq0DAJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "source": [
        "class ScaledDotProductAttention(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(ScaledDotProductAttention, self).__init__()\r\n",
        "\r\n",
        "    def forward(self, Q, K, V, attn_mask):\r\n",
        "        '''\r\n",
        "        Q: [batch_size, n_heads, len_q, d_k]\r\n",
        "        K: [batch_size, n_heads, len_k, d_k]\r\n",
        "        V: [batch_size, n_heads, len_v(=len_k), d_v]\r\n",
        "        attn_mask: [batch_size, n_heads, seq_len, seq_len]\r\n",
        "        '''\r\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size, n_heads, len_q, len_k]\r\n",
        "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is True.\r\n",
        "        \r\n",
        "        attn = nn.Softmax(dim=-1)(scores)\r\n",
        "        context = torch.matmul(attn, V) # [batch_size, n_heads, len_q, d_v]\r\n",
        "        return context, attn\r\n",
        "\r\n",
        "class MultiHeadAttention(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(MultiHeadAttention, self).__init__()\r\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\r\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\r\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\r\n",
        "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\r\n",
        "    def forward(self, input_Q, input_K, input_V, attn_mask):\r\n",
        "        '''\r\n",
        "        input_Q: [batch_size, len_q, d_model]\r\n",
        "        input_K: [batch_size, len_k, d_model]\r\n",
        "        input_V: [batch_size, len_v(=len_k), d_model]\r\n",
        "        attn_mask: [batch_size, seq_len, seq_len]\r\n",
        "        '''\r\n",
        "        residual, batch_size = input_Q, input_Q.size(0)\r\n",
        "        # (B, S, D) -proj-> (B, S, D_new) -split-> (B, S, H, W) -trans-> (B, H, S, W)\r\n",
        "        Q = self.W_Q(input_Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # Q: [batch_size, n_heads, len_q, d_k]\r\n",
        "        K = self.W_K(input_K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # K: [batch_size, n_heads, len_k, d_k]\r\n",
        "        V = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # V: [batch_size, n_heads, len_v(=len_k), d_v]\r\n",
        "\r\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size, n_heads, seq_len, seq_len]\r\n",
        "\r\n",
        "        # context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]\r\n",
        "        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)\r\n",
        "        context = context.transpose(1, 2).reshape(batch_size, -1, n_heads * d_v) # context: [batch_size, len_q, n_heads * d_v]\r\n",
        "        output = self.fc(context) # [batch_size, len_q, d_model]\r\n",
        "        return nn.LayerNorm(d_model).cuda()(output + residual), attn\r\n",
        "\r\n",
        "class PoswiseFeedForwardNet(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\r\n",
        "        self.fc = nn.Sequential(\r\n",
        "            nn.Linear(d_model, d_ff, bias=False),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Linear(d_ff, d_model, bias=False)\r\n",
        "        )\r\n",
        "    def forward(self, inputs):\r\n",
        "        '''\r\n",
        "        inputs: [batch_size, seq_len, d_model]\r\n",
        "        '''\r\n",
        "        residual = inputs\r\n",
        "        output = self.fc(inputs)\r\n",
        "        return nn.LayerNorm(d_model).cuda()(output + residual) # [batch_size, seq_len, d_model]\r\n",
        "\r\n",
        "class EncoderLayer(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(EncoderLayer, self).__init__()\r\n",
        "        # self.enc_self_attn = MultiHeadAttention()\r\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\r\n",
        "\r\n",
        "    def forward(self, enc_inputs):\r\n",
        "        '''\r\n",
        "        enc_inputs: [batch_size, src_len, d_model]\r\n",
        "        enc_self_attn_mask: [batch_size, src_len, src_len]\r\n",
        "        '''\r\n",
        "        # enc_outputs: [batch_size, src_len, d_model], attn: [batch_size, n_heads, src_len, src_len]\r\n",
        "        #enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\r\n",
        "        enc_outputs = self.pos_ffn(enc_inputs) # enc_outputs: [batch_size, src_len, d_model]\r\n",
        "        return enc_outputs\r\n",
        "\r\n",
        "class DecoderLayer(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(DecoderLayer, self).__init__()\r\n",
        "        self.dec_self_attn = MultiHeadAttention()\r\n",
        "        self.dec_enc_attn = MultiHeadAttention()\r\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\r\n",
        "\r\n",
        "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\r\n",
        "        '''\r\n",
        "        dec_inputs: [batch_size, tgt_len, d_model]\r\n",
        "        enc_outputs: [batch_size, src_len, d_model]\r\n",
        "        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\r\n",
        "        dec_enc_attn_mask: [batch_size, tgt_len, src_len]\r\n",
        "        '''\r\n",
        "        # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]\r\n",
        "        #经过一次self-attention\r\n",
        "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\r\n",
        "\r\n",
        "        # dec_outputs: [batch_size, tgt_len, d_model], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\r\n",
        "        #将self-attention的output作为q,encoder的output作为k.v的输入进行再一次attention\r\n",
        "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\r\n",
        "        dec_outputs = self.pos_ffn(dec_outputs) # [batch_size, tgt_len, d_model]\r\n",
        "        return dec_outputs, dec_self_attn, dec_enc_attn\r\n",
        "\r\n",
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Encoder, self).__init__()\r\n",
        "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\r\n",
        "        self.pos_emb = PositionalEncoding(d_model)\r\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\r\n",
        "\r\n",
        "    def forward(self, enc_inputs):\r\n",
        "        '''\r\n",
        "        enc_inputs: [batch_size, src_len]\r\n",
        "        '''\r\n",
        "        enc_outputs = self.src_emb(enc_inputs) # [batch_size, src_len, d_model]#################d_model为词向量长度\r\n",
        "        enc_outputs = self.pos_emb(enc_outputs.transpose(0, 1)).transpose(0, 1) # [batch_size, src_len, d_model]\r\n",
        "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs) # [batch_size, src_len, src_len]\r\n",
        "        #enc_self_attns = []\r\n",
        "        for layer in self.layers:\r\n",
        "            # enc_outputs: [batch_size, src_len, d_model], enc_self_attn: [batch_size, n_heads, src_len, src_len]\r\n",
        "            enc_outputs = layer(enc_outputs)\r\n",
        "            #enc_self_attns.append(enc_self_attn)\r\n",
        "        #print(enc_self_attns)#############1,一个疑问，这是什么\r\n",
        "        return enc_outputs\r\n",
        "\r\n",
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Decoder, self).__init__()\r\n",
        "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\r\n",
        "        self.pos_emb = PositionalEncoding(d_model)\r\n",
        "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\r\n",
        "\r\n",
        "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\r\n",
        "        '''\r\n",
        "        dec_inputs: [batch_size, tgt_len]\r\n",
        "        enc_inputs: [batch_size, src_len]\r\n",
        "        enc_outputs: [batsh_size, src_len, d_model]\r\n",
        "        '''\r\n",
        "        #给单词编码\r\n",
        "        dec_outputs = self.tgt_emb(dec_inputs) # [batch_size, tgt_len, d_model]\r\n",
        "      \r\n",
        "        dec_outputs = self.pos_emb(dec_outputs.transpose(0, 1)).transpose(0, 1).cuda() # [batch_size, tgt_len, d_model]\r\n",
        "\r\n",
        "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).cuda() # [batch_size, tgt_len, tgt_len]\r\n",
        "        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).cuda() # [batch_size, tgt_len, tgt_len]#####一个主要作用是屏蔽未来时刻单词的信息。\r\n",
        "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask), 0).cuda() # [batch_size, tgt_len, tgt_len]【true false\r\n",
        "        #以上几行mask掉了未来时刻和pad的信息\r\n",
        "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs) # [batc_size, tgt_len, src_len]\r\n",
        "        #tensor([[[False, False, False, False,  True],\r\n",
        "        #  [False, False, False, False,  True],\r\n",
        "        #  [False, False, False, False,  True],\r\n",
        "        #  [False, False, False, False,  True],\r\n",
        "        #  [False, False, False, False,  True],\r\n",
        "        #  [False, False, False, False,  True]],\r\n",
        "\r\n",
        "        # [[False, False, False, False,  True],\r\n",
        "        #  [False, False, False, False,  True],\r\n",
        "        #  [False, False, False, False,  True],\r\n",
        "        #  [False, False, False, False,  True],\r\n",
        "        #  [False, False, False, False,  True],\r\n",
        "        #  [False, False, False, False,  True]]])\r\n",
        "\r\n",
        "        dec_self_attns, dec_enc_attns = [], []\r\n",
        "        for layer in self.layers:\r\n",
        "            # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\r\n",
        "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)#######问题3这步里面是啥\r\n",
        "            dec_self_attns.append(dec_self_attn)\r\n",
        "            dec_enc_attns.append(dec_enc_attn)\r\n",
        "        return dec_outputs, dec_self_attns, dec_enc_attns\r\n",
        "\r\n",
        "class Transformer(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Transformer, self).__init__()\r\n",
        "        self.encoder = Encoder().cuda()\r\n",
        "        self.decoder = Decoder().cuda()\r\n",
        "        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False).cuda()\r\n",
        "    def forward(self, enc_inputs, dec_inputs):\r\n",
        "        '''\r\n",
        "        enc_inputs: [batch_size, src_len]\r\n",
        "        dec_inputs: [batch_size, tgt_len]\r\n",
        "        '''\r\n",
        "        # tensor to store decoder outputs\r\n",
        "        # outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\r\n",
        "        \r\n",
        "        # enc_outputs: [batch_size, src_len, d_model], enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]\r\n",
        "        enc_outputs= self.encoder(enc_inputs)\r\n",
        "        # dec_outpus: [batch_size, tgt_len, d_model], dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]\r\n",
        "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\r\n",
        "        dec_logits = self.projection(dec_outputs) # dec_logits: [batch_size, tgt_len, tgt_vocab_size]\r\n",
        "        #print(dec_logits)\r\n",
        "        return dec_logits.view(-1, dec_logits.size(-1)),  dec_self_attns, dec_enc_attns\r\n",
        "\r\n",
        "model = Transformer().cuda()\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\r\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\r\n",
        "#optimizer = optim.Adam(model.parameters(), lr=1e-2)\r\n",
        "# ckpt=torch.load(\"para.pth\")\r\n",
        "# model.load_state_dict(ckpt)\r\n",
        "loss_list=[]\r\n",
        "xx=[]\r\n",
        "min_loss=1e9"
      ],
      "outputs": [],
      "metadata": {
        "id": "kax-QdvK0PCZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "source": [
        "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\r\n",
        "\r\n",
        "for epoch in range(500):\r\n",
        "  \r\n",
        "    for enc_inputs, dec_inputs, dec_outputs in train_loader:\r\n",
        "      '''\r\n",
        "      enc_inputs: [batch_size, src_len]\r\n",
        "      dec_inputs: [batch_size, tgt_len]\r\n",
        "      dec_outputs: [batch_size, tgt_len]\r\n",
        "      '''\r\n",
        "      enc_inputs, dec_inputs, dec_outputs = enc_inputs.cuda(), dec_inputs.cuda(), dec_outputs.cuda()\r\n",
        "      # outputs: [batch_size * tgt_len, tgt_vocab_size]\r\n",
        "      outputs, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\r\n",
        "      loss = criterion(outputs, dec_outputs.view(-1))\r\n",
        "      \r\n",
        "      optimizer.zero_grad()\r\n",
        "      loss.backward()\r\n",
        "      optimizer.step()\r\n",
        "    # if loss<min_loss:\r\n",
        "    #   torch.save(model.state_dict(),\"para2.pth\")\r\n",
        "    loss_list.append(loss)\r\n",
        "    xx.append(epoch)\r\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\r\n",
        "    # scheduler.step()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss = 0.076606\n",
            "Epoch: 0002 loss = 0.077136\n",
            "Epoch: 0003 loss = 0.074303\n",
            "Epoch: 0004 loss = 0.076730\n",
            "Epoch: 0005 loss = 0.077383\n",
            "Epoch: 0006 loss = 0.078062\n",
            "Epoch: 0007 loss = 0.078558\n",
            "Epoch: 0008 loss = 0.077003\n",
            "Epoch: 0009 loss = 0.077525\n",
            "Epoch: 0010 loss = 0.077897\n",
            "Epoch: 0011 loss = 0.077445\n",
            "Epoch: 0012 loss = 0.077348\n",
            "Epoch: 0013 loss = 0.077066\n",
            "Epoch: 0014 loss = 0.078022\n",
            "Epoch: 0015 loss = 0.076618\n",
            "Epoch: 0016 loss = 0.077735\n",
            "Epoch: 0017 loss = 0.077386\n",
            "Epoch: 0018 loss = 0.082147\n",
            "Epoch: 0019 loss = 0.077378\n",
            "Epoch: 0020 loss = 0.078693\n",
            "Epoch: 0021 loss = 0.078346\n",
            "Epoch: 0022 loss = 0.076177\n",
            "Epoch: 0023 loss = 0.078095\n",
            "Epoch: 0024 loss = 0.077677\n",
            "Epoch: 0025 loss = 0.077301\n",
            "Epoch: 0026 loss = 0.076906\n",
            "Epoch: 0027 loss = 0.077633\n",
            "Epoch: 0028 loss = 0.085485\n",
            "Epoch: 0029 loss = 0.077902\n",
            "Epoch: 0030 loss = 0.078533\n",
            "Epoch: 0031 loss = 0.076938\n",
            "Epoch: 0032 loss = 0.087680\n",
            "Epoch: 0033 loss = 0.077523\n",
            "Epoch: 0034 loss = 0.081242\n",
            "Epoch: 0035 loss = 0.077711\n",
            "Epoch: 0036 loss = 0.083554\n",
            "Epoch: 0037 loss = 0.080724\n",
            "Epoch: 0038 loss = 0.084838\n",
            "Epoch: 0039 loss = 0.078811\n",
            "Epoch: 0040 loss = 0.075162\n",
            "Epoch: 0041 loss = 0.075044\n",
            "Epoch: 0042 loss = 0.078011\n",
            "Epoch: 0043 loss = 0.082162\n",
            "Epoch: 0044 loss = 0.083307\n",
            "Epoch: 0045 loss = 0.077888\n",
            "Epoch: 0046 loss = 0.081332\n",
            "Epoch: 0047 loss = 0.075203\n",
            "Epoch: 0048 loss = 0.074552\n",
            "Epoch: 0049 loss = 0.078704\n",
            "Epoch: 0050 loss = 0.074154\n",
            "Epoch: 0051 loss = 0.076353\n",
            "Epoch: 0052 loss = 0.076577\n",
            "Epoch: 0053 loss = 0.076425\n",
            "Epoch: 0054 loss = 0.076663\n",
            "Epoch: 0055 loss = 0.076484\n",
            "Epoch: 0056 loss = 0.076582\n",
            "Epoch: 0057 loss = 0.076250\n",
            "Epoch: 0058 loss = 0.077181\n",
            "Epoch: 0059 loss = 0.077054\n",
            "Epoch: 0060 loss = 0.076975\n",
            "Epoch: 0061 loss = 0.077242\n",
            "Epoch: 0062 loss = 0.077060\n",
            "Epoch: 0063 loss = 0.077783\n",
            "Epoch: 0064 loss = 0.078526\n",
            "Epoch: 0065 loss = 0.081238\n",
            "Epoch: 0066 loss = 0.079837\n",
            "Epoch: 0067 loss = 0.077329\n",
            "Epoch: 0068 loss = 0.074889\n",
            "Epoch: 0069 loss = 0.077328\n",
            "Epoch: 0070 loss = 0.076793\n",
            "Epoch: 0071 loss = 0.076835\n",
            "Epoch: 0072 loss = 0.077049\n",
            "Epoch: 0073 loss = 0.076989\n",
            "Epoch: 0074 loss = 0.077865\n",
            "Epoch: 0075 loss = 0.079426\n",
            "Epoch: 0076 loss = 0.076587\n",
            "Epoch: 0077 loss = 0.077652\n",
            "Epoch: 0078 loss = 0.078007\n",
            "Epoch: 0079 loss = 0.072825\n",
            "Epoch: 0080 loss = 0.078284\n",
            "Epoch: 0081 loss = 0.077523\n",
            "Epoch: 0082 loss = 0.076749\n",
            "Epoch: 0083 loss = 0.076850\n",
            "Epoch: 0084 loss = 0.078789\n",
            "Epoch: 0085 loss = 0.077459\n",
            "Epoch: 0086 loss = 0.077866\n",
            "Epoch: 0087 loss = 0.078419\n",
            "Epoch: 0088 loss = 0.078059\n",
            "Epoch: 0089 loss = 0.079528\n",
            "Epoch: 0090 loss = 0.086536\n",
            "Epoch: 0091 loss = 0.077988\n",
            "Epoch: 0092 loss = 0.078974\n",
            "Epoch: 0093 loss = 0.076689\n",
            "Epoch: 0094 loss = 0.076099\n",
            "Epoch: 0095 loss = 0.078190\n",
            "Epoch: 0096 loss = 0.076551\n",
            "Epoch: 0097 loss = 0.077501\n",
            "Epoch: 0098 loss = 0.076350\n",
            "Epoch: 0099 loss = 0.083021\n",
            "Epoch: 0100 loss = 0.079933\n",
            "Epoch: 0101 loss = 0.076947\n",
            "Epoch: 0102 loss = 0.076941\n",
            "Epoch: 0103 loss = 0.084178\n",
            "Epoch: 0104 loss = 0.076770\n",
            "Epoch: 0105 loss = 0.078201\n",
            "Epoch: 0106 loss = 0.077539\n",
            "Epoch: 0107 loss = 0.073086\n",
            "Epoch: 0108 loss = 0.077642\n",
            "Epoch: 0109 loss = 0.075930\n",
            "Epoch: 0110 loss = 0.079791\n",
            "Epoch: 0111 loss = 0.081652\n",
            "Epoch: 0112 loss = 0.078539\n",
            "Epoch: 0113 loss = 0.073518\n",
            "Epoch: 0114 loss = 0.081471\n",
            "Epoch: 0115 loss = 0.078496\n",
            "Epoch: 0116 loss = 0.079976\n",
            "Epoch: 0117 loss = 0.078906\n",
            "Epoch: 0118 loss = 0.075698\n",
            "Epoch: 0119 loss = 0.077690\n",
            "Epoch: 0120 loss = 0.077192\n",
            "Epoch: 0121 loss = 0.077082\n",
            "Epoch: 0122 loss = 0.077309\n",
            "Epoch: 0123 loss = 0.077366\n",
            "Epoch: 0124 loss = 0.075975\n",
            "Epoch: 0125 loss = 0.077436\n",
            "Epoch: 0126 loss = 0.077348\n",
            "Epoch: 0127 loss = 0.078718\n",
            "Epoch: 0128 loss = 0.077151\n",
            "Epoch: 0129 loss = 0.077181\n",
            "Epoch: 0130 loss = 0.078000\n",
            "Epoch: 0131 loss = 0.075851\n",
            "Epoch: 0132 loss = 0.077370\n",
            "Epoch: 0133 loss = 0.077349\n",
            "Epoch: 0134 loss = 0.078278\n",
            "Epoch: 0135 loss = 0.078881\n",
            "Epoch: 0136 loss = 0.079426\n",
            "Epoch: 0137 loss = 0.076976\n",
            "Epoch: 0138 loss = 0.080501\n",
            "Epoch: 0139 loss = 0.078836\n",
            "Epoch: 0140 loss = 0.086773\n",
            "Epoch: 0141 loss = 0.078200\n",
            "Epoch: 0142 loss = 0.075154\n",
            "Epoch: 0143 loss = 0.077368\n",
            "Epoch: 0144 loss = 0.079093\n",
            "Epoch: 0145 loss = 0.076583\n",
            "Epoch: 0146 loss = 0.076539\n",
            "Epoch: 0147 loss = 0.076502\n",
            "Epoch: 0148 loss = 0.075650\n",
            "Epoch: 0149 loss = 0.078384\n",
            "Epoch: 0150 loss = 0.077342\n",
            "Epoch: 0151 loss = 0.077851\n",
            "Epoch: 0152 loss = 0.079149\n",
            "Epoch: 0153 loss = 0.077534\n",
            "Epoch: 0154 loss = 0.074765\n",
            "Epoch: 0155 loss = 0.078734\n",
            "Epoch: 0156 loss = 0.079807\n",
            "Epoch: 0157 loss = 0.079487\n",
            "Epoch: 0158 loss = 0.079696\n",
            "Epoch: 0159 loss = 0.077798\n",
            "Epoch: 0160 loss = 0.076183\n",
            "Epoch: 0161 loss = 0.075807\n",
            "Epoch: 0162 loss = 0.078549\n",
            "Epoch: 0163 loss = 0.076882\n",
            "Epoch: 0164 loss = 0.077291\n",
            "Epoch: 0165 loss = 0.076712\n",
            "Epoch: 0166 loss = 0.077096\n",
            "Epoch: 0167 loss = 0.077121\n",
            "Epoch: 0168 loss = 0.076731\n",
            "Epoch: 0169 loss = 0.077386\n",
            "Epoch: 0170 loss = 0.078018\n",
            "Epoch: 0171 loss = 0.077618\n",
            "Epoch: 0172 loss = 0.077452\n",
            "Epoch: 0173 loss = 0.076849\n",
            "Epoch: 0174 loss = 0.077288\n",
            "Epoch: 0175 loss = 0.076781\n",
            "Epoch: 0176 loss = 0.076856\n",
            "Epoch: 0177 loss = 0.075415\n",
            "Epoch: 0178 loss = 0.076655\n",
            "Epoch: 0179 loss = 0.076650\n",
            "Epoch: 0180 loss = 0.080196\n",
            "Epoch: 0181 loss = 0.077212\n",
            "Epoch: 0182 loss = 0.078622\n",
            "Epoch: 0183 loss = 0.076685\n",
            "Epoch: 0184 loss = 0.077886\n",
            "Epoch: 0185 loss = 0.076605\n",
            "Epoch: 0186 loss = 0.077915\n",
            "Epoch: 0187 loss = 0.079136\n",
            "Epoch: 0188 loss = 0.076695\n",
            "Epoch: 0189 loss = 0.076907\n",
            "Epoch: 0190 loss = 0.076306\n",
            "Epoch: 0191 loss = 0.076411\n",
            "Epoch: 0192 loss = 0.078038\n",
            "Epoch: 0193 loss = 0.076223\n",
            "Epoch: 0194 loss = 0.076536\n",
            "Epoch: 0195 loss = 0.079467\n",
            "Epoch: 0196 loss = 0.077770\n",
            "Epoch: 0197 loss = 0.078662\n",
            "Epoch: 0198 loss = 0.078588\n",
            "Epoch: 0199 loss = 0.083860\n",
            "Epoch: 0200 loss = 0.074859\n",
            "Epoch: 0201 loss = 0.082701\n",
            "Epoch: 0202 loss = 0.076164\n",
            "Epoch: 0203 loss = 0.081712\n",
            "Epoch: 0204 loss = 0.080963\n",
            "Epoch: 0205 loss = 0.085751\n",
            "Epoch: 0206 loss = 0.077249\n",
            "Epoch: 0207 loss = 0.082615\n",
            "Epoch: 0208 loss = 0.075190\n",
            "Epoch: 0209 loss = 0.077126\n",
            "Epoch: 0210 loss = 0.079440\n",
            "Epoch: 0211 loss = 0.083003\n",
            "Epoch: 0212 loss = 0.077062\n",
            "Epoch: 0213 loss = 0.076248\n",
            "Epoch: 0214 loss = 0.079856\n",
            "Epoch: 0215 loss = 0.076547\n",
            "Epoch: 0216 loss = 0.080900\n",
            "Epoch: 0217 loss = 0.076969\n",
            "Epoch: 0218 loss = 0.075062\n",
            "Epoch: 0219 loss = 0.077640\n",
            "Epoch: 0220 loss = 0.077484\n",
            "Epoch: 0221 loss = 0.078175\n",
            "Epoch: 0222 loss = 0.076001\n",
            "Epoch: 0223 loss = 0.077264\n",
            "Epoch: 0224 loss = 0.076693\n",
            "Epoch: 0225 loss = 0.080087\n",
            "Epoch: 0226 loss = 0.076243\n",
            "Epoch: 0227 loss = 0.074079\n",
            "Epoch: 0228 loss = 0.077782\n",
            "Epoch: 0229 loss = 0.077310\n",
            "Epoch: 0230 loss = 0.077753\n",
            "Epoch: 0231 loss = 0.079223\n",
            "Epoch: 0232 loss = 0.077103\n",
            "Epoch: 0233 loss = 0.076822\n",
            "Epoch: 0234 loss = 0.079311\n",
            "Epoch: 0235 loss = 0.076807\n",
            "Epoch: 0236 loss = 0.079618\n",
            "Epoch: 0237 loss = 0.077034\n",
            "Epoch: 0238 loss = 0.075287\n",
            "Epoch: 0239 loss = 0.079477\n",
            "Epoch: 0240 loss = 0.081242\n",
            "Epoch: 0241 loss = 0.077257\n",
            "Epoch: 0242 loss = 0.075127\n",
            "Epoch: 0243 loss = 0.080262\n",
            "Epoch: 0244 loss = 0.077883\n",
            "Epoch: 0245 loss = 0.077450\n",
            "Epoch: 0246 loss = 0.077814\n",
            "Epoch: 0247 loss = 0.074262\n",
            "Epoch: 0248 loss = 0.079111\n",
            "Epoch: 0249 loss = 0.076658\n",
            "Epoch: 0250 loss = 0.081461\n",
            "Epoch: 0251 loss = 0.076957\n",
            "Epoch: 0252 loss = 0.081828\n",
            "Epoch: 0253 loss = 0.077528\n",
            "Epoch: 0254 loss = 0.076720\n",
            "Epoch: 0255 loss = 0.077745\n",
            "Epoch: 0256 loss = 0.077874\n",
            "Epoch: 0257 loss = 0.077171\n",
            "Epoch: 0258 loss = 0.075821\n",
            "Epoch: 0259 loss = 0.077317\n",
            "Epoch: 0260 loss = 0.073462\n",
            "Epoch: 0261 loss = 0.079912\n",
            "Epoch: 0262 loss = 0.075361\n",
            "Epoch: 0263 loss = 0.076667\n",
            "Epoch: 0264 loss = 0.080220\n",
            "Epoch: 0265 loss = 0.076889\n",
            "Epoch: 0266 loss = 0.076725\n",
            "Epoch: 0267 loss = 0.080706\n",
            "Epoch: 0268 loss = 0.076953\n",
            "Epoch: 0269 loss = 0.078233\n",
            "Epoch: 0270 loss = 0.077731\n",
            "Epoch: 0271 loss = 0.076892\n",
            "Epoch: 0272 loss = 0.079496\n",
            "Epoch: 0273 loss = 0.077193\n",
            "Epoch: 0274 loss = 0.078071\n",
            "Epoch: 0275 loss = 0.078827\n",
            "Epoch: 0276 loss = 0.076540\n",
            "Epoch: 0277 loss = 0.081166\n",
            "Epoch: 0278 loss = 0.077348\n",
            "Epoch: 0279 loss = 0.077009\n",
            "Epoch: 0280 loss = 0.077664\n",
            "Epoch: 0281 loss = 0.075325\n",
            "Epoch: 0282 loss = 0.077521\n",
            "Epoch: 0283 loss = 0.075983\n",
            "Epoch: 0284 loss = 0.077420\n",
            "Epoch: 0285 loss = 0.075594\n",
            "Epoch: 0286 loss = 0.075935\n",
            "Epoch: 0287 loss = 0.077693\n",
            "Epoch: 0288 loss = 0.083170\n",
            "Epoch: 0289 loss = 0.078725\n",
            "Epoch: 0290 loss = 0.080305\n",
            "Epoch: 0291 loss = 0.076602\n",
            "Epoch: 0292 loss = 0.076655\n",
            "Epoch: 0293 loss = 0.076977\n",
            "Epoch: 0294 loss = 0.077316\n",
            "Epoch: 0295 loss = 0.077492\n",
            "Epoch: 0296 loss = 0.077143\n",
            "Epoch: 0297 loss = 0.076452\n",
            "Epoch: 0298 loss = 0.078465\n",
            "Epoch: 0299 loss = 0.076366\n",
            "Epoch: 0300 loss = 0.077689\n",
            "Epoch: 0301 loss = 0.075983\n",
            "Epoch: 0302 loss = 0.078045\n",
            "Epoch: 0303 loss = 0.081194\n",
            "Epoch: 0304 loss = 0.077454\n",
            "Epoch: 0305 loss = 0.079109\n",
            "Epoch: 0306 loss = 0.077217\n",
            "Epoch: 0307 loss = 0.076647\n",
            "Epoch: 0308 loss = 0.077172\n",
            "Epoch: 0309 loss = 0.077249\n",
            "Epoch: 0310 loss = 0.078461\n",
            "Epoch: 0311 loss = 0.077041\n",
            "Epoch: 0312 loss = 0.076095\n",
            "Epoch: 0313 loss = 0.076899\n",
            "Epoch: 0314 loss = 0.077558\n",
            "Epoch: 0315 loss = 0.078678\n",
            "Epoch: 0316 loss = 0.078818\n",
            "Epoch: 0317 loss = 0.076400\n",
            "Epoch: 0318 loss = 0.077144\n",
            "Epoch: 0319 loss = 0.079136\n",
            "Epoch: 0320 loss = 0.076325\n",
            "Epoch: 0321 loss = 0.076715\n",
            "Epoch: 0322 loss = 0.077038\n",
            "Epoch: 0323 loss = 0.077262\n",
            "Epoch: 0324 loss = 0.077697\n",
            "Epoch: 0325 loss = 0.077862\n",
            "Epoch: 0326 loss = 0.076821\n",
            "Epoch: 0327 loss = 0.078034\n",
            "Epoch: 0328 loss = 0.076953\n",
            "Epoch: 0329 loss = 0.079490\n",
            "Epoch: 0330 loss = 0.076318\n",
            "Epoch: 0331 loss = 0.082859\n",
            "Epoch: 0332 loss = 0.079380\n",
            "Epoch: 0333 loss = 0.080055\n",
            "Epoch: 0334 loss = 0.076561\n",
            "Epoch: 0335 loss = 0.077245\n",
            "Epoch: 0336 loss = 0.077704\n",
            "Epoch: 0337 loss = 0.078755\n",
            "Epoch: 0338 loss = 0.077102\n",
            "Epoch: 0339 loss = 0.078188\n",
            "Epoch: 0340 loss = 0.078282\n",
            "Epoch: 0341 loss = 0.076544\n",
            "Epoch: 0342 loss = 0.076827\n",
            "Epoch: 0343 loss = 0.078126\n",
            "Epoch: 0344 loss = 0.076796\n",
            "Epoch: 0345 loss = 0.076262\n",
            "Epoch: 0346 loss = 0.077306\n",
            "Epoch: 0347 loss = 0.077481\n",
            "Epoch: 0348 loss = 0.077099\n",
            "Epoch: 0349 loss = 0.077540\n",
            "Epoch: 0350 loss = 0.077383\n",
            "Epoch: 0351 loss = 0.079036\n",
            "Epoch: 0352 loss = 0.077951\n",
            "Epoch: 0353 loss = 0.077829\n",
            "Epoch: 0354 loss = 0.078652\n",
            "Epoch: 0355 loss = 0.078591\n",
            "Epoch: 0356 loss = 0.078012\n",
            "Epoch: 0357 loss = 0.078163\n",
            "Epoch: 0358 loss = 0.081244\n",
            "Epoch: 0359 loss = 0.077873\n",
            "Epoch: 0360 loss = 0.079309\n",
            "Epoch: 0361 loss = 0.077316\n",
            "Epoch: 0362 loss = 0.082625\n",
            "Epoch: 0363 loss = 0.077645\n",
            "Epoch: 0364 loss = 0.080203\n",
            "Epoch: 0365 loss = 0.077158\n",
            "Epoch: 0366 loss = 0.078402\n",
            "Epoch: 0367 loss = 0.078118\n",
            "Epoch: 0368 loss = 0.084543\n",
            "Epoch: 0369 loss = 0.083052\n",
            "Epoch: 0370 loss = 0.078558\n",
            "Epoch: 0371 loss = 0.080389\n",
            "Epoch: 0372 loss = 0.075557\n",
            "Epoch: 0373 loss = 0.079896\n",
            "Epoch: 0374 loss = 0.077731\n",
            "Epoch: 0375 loss = 0.085430\n",
            "Epoch: 0376 loss = 0.076945\n",
            "Epoch: 0377 loss = 0.076903\n",
            "Epoch: 0378 loss = 0.076356\n",
            "Epoch: 0379 loss = 0.077461\n",
            "Epoch: 0380 loss = 0.081718\n",
            "Epoch: 0381 loss = 0.084397\n",
            "Epoch: 0382 loss = 0.075335\n",
            "Epoch: 0383 loss = 0.078439\n",
            "Epoch: 0384 loss = 0.075382\n",
            "Epoch: 0385 loss = 0.080822\n",
            "Epoch: 0386 loss = 0.074206\n",
            "Epoch: 0387 loss = 0.076165\n",
            "Epoch: 0388 loss = 0.078706\n",
            "Epoch: 0389 loss = 0.076501\n",
            "Epoch: 0390 loss = 0.080830\n",
            "Epoch: 0391 loss = 0.077231\n",
            "Epoch: 0392 loss = 0.084486\n",
            "Epoch: 0393 loss = 0.078056\n",
            "Epoch: 0394 loss = 0.077706\n",
            "Epoch: 0395 loss = 0.077828\n",
            "Epoch: 0396 loss = 0.074619\n",
            "Epoch: 0397 loss = 0.077698\n",
            "Epoch: 0398 loss = 0.079120\n",
            "Epoch: 0399 loss = 0.076717\n",
            "Epoch: 0400 loss = 0.077172\n",
            "Epoch: 0401 loss = 0.077007\n",
            "Epoch: 0402 loss = 0.078492\n",
            "Epoch: 0403 loss = 0.077192\n",
            "Epoch: 0404 loss = 0.076923\n",
            "Epoch: 0405 loss = 0.081256\n",
            "Epoch: 0406 loss = 0.077597\n",
            "Epoch: 0407 loss = 0.080201\n",
            "Epoch: 0408 loss = 0.077924\n",
            "Epoch: 0409 loss = 0.075989\n",
            "Epoch: 0410 loss = 0.075790\n",
            "Epoch: 0411 loss = 0.076480\n",
            "Epoch: 0412 loss = 0.077477\n",
            "Epoch: 0413 loss = 0.076882\n",
            "Epoch: 0414 loss = 0.079032\n",
            "Epoch: 0415 loss = 0.076949\n",
            "Epoch: 0416 loss = 0.077892\n",
            "Epoch: 0417 loss = 0.077095\n",
            "Epoch: 0418 loss = 0.077102\n",
            "Epoch: 0419 loss = 0.076452\n",
            "Epoch: 0420 loss = 0.076276\n",
            "Epoch: 0421 loss = 0.076774\n",
            "Epoch: 0422 loss = 0.077879\n",
            "Epoch: 0423 loss = 0.077751\n",
            "Epoch: 0424 loss = 0.077015\n",
            "Epoch: 0425 loss = 0.076364\n",
            "Epoch: 0426 loss = 0.077398\n",
            "Epoch: 0427 loss = 0.077361\n",
            "Epoch: 0428 loss = 0.078476\n",
            "Epoch: 0429 loss = 0.077767\n",
            "Epoch: 0430 loss = 0.076933\n",
            "Epoch: 0431 loss = 0.077931\n",
            "Epoch: 0432 loss = 0.076166\n",
            "Epoch: 0433 loss = 0.078030\n",
            "Epoch: 0434 loss = 0.079758\n",
            "Epoch: 0435 loss = 0.076493\n",
            "Epoch: 0436 loss = 0.076328\n",
            "Epoch: 0437 loss = 0.077002\n",
            "Epoch: 0438 loss = 0.078286\n",
            "Epoch: 0439 loss = 0.076397\n",
            "Epoch: 0440 loss = 0.079472\n",
            "Epoch: 0441 loss = 0.077517\n",
            "Epoch: 0442 loss = 0.078900\n",
            "Epoch: 0443 loss = 0.076683\n",
            "Epoch: 0444 loss = 0.077565\n",
            "Epoch: 0445 loss = 0.076874\n",
            "Epoch: 0446 loss = 0.075919\n",
            "Epoch: 0447 loss = 0.079814\n",
            "Epoch: 0448 loss = 0.075080\n",
            "Epoch: 0449 loss = 0.078866\n",
            "Epoch: 0450 loss = 0.079059\n",
            "Epoch: 0451 loss = 0.078736\n",
            "Epoch: 0452 loss = 0.080092\n",
            "Epoch: 0453 loss = 0.073946\n",
            "Epoch: 0454 loss = 0.076975\n",
            "Epoch: 0455 loss = 0.077939\n",
            "Epoch: 0456 loss = 0.076799\n",
            "Epoch: 0457 loss = 0.084059\n",
            "Epoch: 0458 loss = 0.077455\n",
            "Epoch: 0459 loss = 0.077974\n",
            "Epoch: 0460 loss = 0.078164\n",
            "Epoch: 0461 loss = 0.081156\n",
            "Epoch: 0462 loss = 0.078102\n",
            "Epoch: 0463 loss = 0.078053\n",
            "Epoch: 0464 loss = 0.078893\n",
            "Epoch: 0465 loss = 0.077490\n",
            "Epoch: 0466 loss = 0.073713\n",
            "Epoch: 0467 loss = 0.077458\n",
            "Epoch: 0468 loss = 0.078863\n",
            "Epoch: 0469 loss = 0.074966\n",
            "Epoch: 0470 loss = 0.075637\n",
            "Epoch: 0471 loss = 0.083605\n",
            "Epoch: 0472 loss = 0.077157\n",
            "Epoch: 0473 loss = 0.075407\n",
            "Epoch: 0474 loss = 0.076636\n",
            "Epoch: 0475 loss = 0.076840\n",
            "Epoch: 0476 loss = 0.077969\n",
            "Epoch: 0477 loss = 0.076895\n",
            "Epoch: 0478 loss = 0.078710\n",
            "Epoch: 0479 loss = 0.077302\n",
            "Epoch: 0480 loss = 0.075794\n",
            "Epoch: 0481 loss = 0.077143\n",
            "Epoch: 0482 loss = 0.076278\n",
            "Epoch: 0483 loss = 0.079858\n",
            "Epoch: 0484 loss = 0.078326\n",
            "Epoch: 0485 loss = 0.079026\n",
            "Epoch: 0486 loss = 0.077835\n",
            "Epoch: 0487 loss = 0.076178\n",
            "Epoch: 0488 loss = 0.076726\n",
            "Epoch: 0489 loss = 0.075137\n",
            "Epoch: 0490 loss = 0.076871\n",
            "Epoch: 0491 loss = 0.078588\n",
            "Epoch: 0492 loss = 0.078006\n",
            "Epoch: 0493 loss = 0.075455\n",
            "Epoch: 0494 loss = 0.078534\n",
            "Epoch: 0495 loss = 0.077311\n",
            "Epoch: 0496 loss = 0.076873\n",
            "Epoch: 0497 loss = 0.080110\n",
            "Epoch: 0498 loss = 0.077040\n",
            "Epoch: 0499 loss = 0.076689\n",
            "Epoch: 0500 loss = 0.077206\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh8S1RRm08uY",
        "outputId": "3242ade9-58a2-4b55-85aa-111cd40af34f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "source": [
        "\r\n",
        "\r\n",
        "for i in range(len(xx)):\r\n",
        "  xx[i]=i\r\n",
        "# for i in range(120,150):\r\n",
        "#   xx[i]=xx[i]+120\r\n",
        "# # for i in range(20,60):\r\n",
        "# #   xx[i]=xx[i]-40\r\n",
        "# # for i in range(60,110):\r\n",
        "# #   xx[i]=xx[i]+100\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "xKYCwfc9VlXN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "source": [
        "\r\n",
        "plt.figure()\r\n",
        "plt.plot(xx,loss_list)\r\n",
        "plt.xlabel('epoch') \r\n",
        "plt.ylabel('loss')\r\n",
        "plt.show()\r\n",
        "l=[]\r\n",
        "for i in range(len(loss_list)):\r\n",
        "  l.append(float(loss_list[i].data))\r\n",
        "with open(\"loss9.txt\",\"w\")as f:\r\n",
        "  print(l,file=f)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "K5E7hXQ_OV4m",
        "outputId": "181a2e9a-bdec-481d-8147-e0c91607c122"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "source": [
        "#function for creating decoder input for test\r\n",
        "def greedy_decoder(model, enc_input, start_symbol):\r\n",
        "    \"\"\"\r\n",
        "    For simplicity, a Greedy Decoder is Beam search when K=1. This is necessary for inference as we don't know the\r\n",
        "    target sequence input. Therefore we try to generate the target input word by word, then feed it into the transformer.\r\n",
        "    Starting Reference: http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding\r\n",
        "    :param model: Transformer Model\r\n",
        "    :param enc_input: The encoder input\r\n",
        "    :param start_symbol: The start symbol. In this example it is 'S' which corresponds to index 4\r\n",
        "    :return: The target input\r\n",
        "    \"\"\"\r\n",
        "    enc_input=enc_input.cuda()\r\n",
        "    enc_outputs = model.encoder(enc_input)\r\n",
        "    dec_input = torch.zeros(1, 0).type_as(enc_input.data)\r\n",
        "    dec_input=dec_input.cuda()\r\n",
        "    terminal = False\r\n",
        "    next_symbol = start_symbol\r\n",
        "    for i in range(8):  \r\n",
        "        a= torch.tensor([[next_symbol]],dtype=enc_input.dtype)\r\n",
        "        a=a.cuda()      \r\n",
        "        dec_input=torch.cat([dec_input.detach(),a],-1)\r\n",
        "        dec_input=dec_input.cuda()\r\n",
        "        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\r\n",
        "        projected = model.projection(dec_outputs)\r\n",
        "        prob = projected.squeeze(0).max(dim=-1, keepdim=False)[1]\r\n",
        "        next_word = prob.data[-1]\r\n",
        "        next_symbol = next_word\r\n",
        "        #print(next_word)            \r\n",
        "    return dec_input"
      ],
      "outputs": [],
      "metadata": {
        "id": "54eszBfj2ckk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#create test data\r\n",
        "import random\r\n",
        "code=df.code.values\r\n",
        "encode=df.encode.values\r\n",
        "\r\n",
        "# #encode=encode[1:854]\r\n",
        "code=code[1:]\r\n",
        "encode=encode[1:]\r\n",
        "label2=[]\r\n",
        "test2=[]\r\n",
        "for i in range(1000):\r\n",
        "  choose=random.randint(0,255)\r\n",
        "  lab=code[choose]\r\n",
        "\r\n",
        "  te=encode[choose]\r\n",
        "  label2.append(lab)\r\n",
        "  test2.append(te)\r\n",
        "enc_inputs, dec_inputs, dec_outputs = make_data(test2,label2)\r\n",
        "test_data=MyDataSet(enc_inputs, dec_inputs, dec_outputs)\r\n",
        "#test_data=MyDataSet(b1,b2,b3)\r\n",
        "test_loader = Data.DataLoader(test_data,batch_size,True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "GvmwG1SsvQSw",
        "outputId": "351bf7d8-e519-46ac-f67e-a939105a0b41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "  #Test\r\n",
        "acc=0\r\n",
        "cnt=0\r\n",
        "for enc_inputs, _, label in test_loader:\r\n",
        "  #print(enc_inputs[:20])\r\n",
        "  #print(label[:20])\r\n",
        "  #print(label)\r\n",
        "  #print(o\r\n",
        "    correct=0\r\n",
        "    total=0\r\n",
        "    enc_inputs=enc_inputs.cuda()\r\n",
        "    #print(len(enc_inputs))\r\n",
        "    #print(src_vocab)\r\n",
        "    for i in range(len(enc_inputs)):\r\n",
        "        greedy_dec_input = greedy_decoder(model, enc_inputs[i].view(1, -1), start_symbol=tgt_vocab[\"S\"])\r\n",
        "        predict, _, _ = model(enc_inputs[i].view(1, -1), greedy_dec_input)\r\n",
        "        #print(predict)\r\n",
        "        a=label[i][:-1]\r\n",
        "        for j in range(len(a)):\r\n",
        "          a[j]=a[j]-1\r\n",
        "        \r\n",
        "        \r\n",
        "        predict = predict.data.max(1, keepdim=True)[1]\r\n",
        "        b=[idx2word[n.item()] for n in predict.squeeze()]\r\n",
        "        for j in range(8):\r\n",
        "          #print(b[j],a[j])\r\n",
        "          #print(type(a[j]))\r\n",
        "          c=1\r\n",
        "          if b[j]=='0.':\r\n",
        "            c=0\r\n",
        "          if c!=a[j]:\r\n",
        "            cnt=cnt+1\r\n",
        "            print(\"real\",a)\r\n",
        "            print(enc_inputs[i], '->', [idx2word[n.item()] for n in predict.squeeze()])\r\n",
        "          \r\n",
        "        #print(enc_inputs[i], '->', [idx2word[n.item()] for n in predict.squeeze()])\r\n",
        "    \r\n",
        "print(cnt)\r\n",
        "print(\"ber=\",cnt/1000/8)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "real tensor([0, 0, 1, 0, 1, 0, 0, 1])\n",
            "tensor([1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2], device='cuda:0') -> ['1.', '0.', '1.', '0.', '1.', '0.', '0.', '1.']\n",
            "real tensor([0, 0, 1, 1, 1, 0, 0, 0])\n",
            "tensor([1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2], device='cuda:0') -> ['1.', '0.', '1.', '1.', '1.', '0.', '0.', '0.']\n",
            "real tensor([0, 0, 0, 1, 1, 1, 0, 1])\n",
            "tensor([1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 1], device='cuda:0') -> ['1.', '0.', '0.', '1.', '1.', '1.', '0.', '1.']\n",
            "real tensor([0, 1, 1, 0, 1, 1, 0, 1])\n",
            "tensor([1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1], device='cuda:0') -> ['1.', '1.', '1.', '0.', '1.', '1.', '0.', '1.']\n",
            "real tensor([0, 1, 1, 0, 0, 0, 1, 0])\n",
            "tensor([1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2], device='cuda:0') -> ['1.', '1.', '1.', '0.', '0.', '0.', '1.', '0.']\n",
            "real tensor([0, 1, 1, 0, 0, 0, 1, 0])\n",
            "tensor([1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2], device='cuda:0') -> ['1.', '1.', '1.', '0.', '0.', '0.', '1.', '0.']\n",
            "real tensor([0, 0, 0, 1, 0, 1, 0, 1])\n",
            "tensor([1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 2], device='cuda:0') -> ['1.', '0.', '0.', '1.', '0.', '1.', '0.', '1.']\n",
            "real tensor([1, 1, 0, 0, 1, 1, 1, 0])\n",
            "tensor([1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1], device='cuda:0') -> ['0.', '1.', '0.', '0.', '1.', '1.', '1.', '0.']\n",
            "real tensor([0, 1, 0, 0, 0, 0, 1, 1])\n",
            "tensor([1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1], device='cuda:0') -> ['1.', '1.', '0.', '0.', '0.', '0.', '1.', '1.']\n",
            "real tensor([0, 1, 1, 1, 0, 0, 0, 1])\n",
            "tensor([1, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1], device='cuda:0') -> ['1.', '1.', '1.', '1.', '0.', '0.', '0.', '1.']\n",
            "real tensor([1, 0, 0, 0, 0, 1, 1, 0])\n",
            "tensor([1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1], device='cuda:0') -> ['0.', '0.', '0.', '0.', '0.', '1.', '1.', '0.']\n",
            "real tensor([0, 0, 0, 0, 0, 0, 1, 1])\n",
            "tensor([1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2], device='cuda:0') -> ['1.', '0.', '0.', '0.', '0.', '0.', '1.', '1.']\n",
            "real tensor([0, 0, 1, 0, 1, 1, 0, 1])\n",
            "tensor([1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 2], device='cuda:0') -> ['1.', '0.', '1.', '0.', '1.', '1.', '0.', '1.']\n",
            "real tensor([1, 0, 1, 0, 1, 1, 0, 1])\n",
            "tensor([1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 2], device='cuda:0') -> ['0.', '0.', '1.', '0.', '1.', '1.', '0.', '1.']\n",
            "real tensor([0, 0, 0, 0, 0, 0, 1, 1])\n",
            "tensor([1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2], device='cuda:0') -> ['1.', '0.', '0.', '0.', '0.', '0.', '1.', '1.']\n",
            "real tensor([0, 1, 0, 0, 1, 1, 0, 0])\n",
            "tensor([1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 2], device='cuda:0') -> ['1.', '1.', '0.', '0.', '1.', '1.', '0.', '0.']\n",
            "real tensor([0, 1, 1, 0, 0, 1, 0, 1])\n",
            "tensor([1, 2, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2], device='cuda:0') -> ['1.', '1.', '1.', '0.', '0.', '1.', '0.', '1.']\n",
            "real tensor([0, 1, 0, 0, 1, 1, 1, 0])\n",
            "tensor([1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1], device='cuda:0') -> ['1.', '1.', '0.', '0.', '1.', '1.', '1.', '0.']\n",
            "real tensor([0, 1, 0, 0, 1, 0, 1, 0])\n",
            "tensor([1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1], device='cuda:0') -> ['1.', '1.', '0.', '0.', '1.', '0.', '1.', '0.']\n",
            "real tensor([1, 0, 0, 1, 1, 0, 0, 0])\n",
            "tensor([1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2], device='cuda:0') -> ['0.', '0.', '0.', '1.', '1.', '0.', '0.', '0.']\n",
            "real tensor([0, 1, 1, 0, 1, 0, 0, 1])\n",
            "tensor([1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 1], device='cuda:0') -> ['1.', '1.', '1.', '0.', '1.', '0.', '0.', '1.']\n",
            "real tensor([0, 1, 1, 0, 1, 1, 0, 1])\n",
            "tensor([1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1], device='cuda:0') -> ['1.', '1.', '1.', '0.', '1.', '1.', '0.', '1.']\n",
            "real tensor([0, 0, 1, 0, 1, 0, 1, 1])\n",
            "tensor([1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1], device='cuda:0') -> ['1.', '0.', '1.', '0.', '1.', '0.', '1.', '1.']\n",
            "real tensor([0, 1, 0, 0, 0, 1, 1, 1])\n",
            "tensor([1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1], device='cuda:0') -> ['1.', '1.', '0.', '0.', '0.', '1.', '1.', '1.']\n",
            "real tensor([0, 1, 1, 1, 1, 1, 1, 0])\n",
            "tensor([1, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2], device='cuda:0') -> ['1.', '1.', '1.', '1.', '1.', '1.', '1.', '0.']\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMsPb5CY3KB6",
        "outputId": "4291cecc-6b4c-4961-acce-a9f4f2ed493f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "uKGm2r2ioSx9"
      }
    }
  ]
}