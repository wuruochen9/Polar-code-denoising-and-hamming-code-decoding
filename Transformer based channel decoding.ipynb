{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FINAL2 (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.5 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "18da31f597fec48f49f0c6c9857b5460e182ca09a9da661d2d0e1b3de986e9ca"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#The code is created by wrc and the transformer part and polar code part are created with the help of other's codes"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install py-polar-codes"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkdoRMpyrLXT",
        "outputId": "df39f1cf-43b2-40cf-cb50-9e54687752ec"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install wget"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trVafcPlBfto",
        "outputId": "5127a835-f73b-43c8-f927-3986284f6e70"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import tensorflow.keras.backend as K\r\n",
        "from polarcodes import *\r\n",
        "import pandas as pd\r\n",
        "import os"
      ],
      "outputs": [],
      "metadata": {
        "id": "BBO9Yf7qreKj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzt1V8oDCvmF",
        "outputId": "7f81cdb8-94e3-42a0-dc0f-898eeeba50b6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "path = \"/content/drive/My Drive/Colab Notebooks/\"\r\n",
        "os.chdir(path)\r\n",
        "os.listdir(path)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AKidz8KDUhJ",
        "outputId": "b9957a08-c8a1-4199-e6ea-fc112b38fe00"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "\r\n",
        "def generate_input_data(k):#载入训练数据\r\n",
        "    X = []\r\n",
        "    for i in range(2**k):\r\n",
        "        bin_str = bin(i)[2:].zfill(k)\r\n",
        "        x = []\r\n",
        "        for j in range(k):\r\n",
        "            x.append(int(bin_str[j]))\r\n",
        "        X.append(x)\r\n",
        "    return np.array(X)\r\n",
        "def get_index_set(N, k, snr_db):\r\n",
        "\t  snr = 10**(snr_db/10)\r\n",
        "\t  bhattacharya_param = np.exp(-snr)\r\n",
        "\r\n",
        "\t  leaves = np.zeros(N)\r\n",
        "\t  leaves[0] = bhattacharya_param\r\n",
        "\r\n",
        "\t  for level in range(1, int(np.log2(N))+1):\r\n",
        "\t\t  for i in range(0, int( (2**level)/2 )):\r\n",
        "\t\t\t  p = leaves[i]\r\n",
        "\t\t\t  leaves[i] = 2*p - p**2\r\n",
        "\t\t\t  leaves[i + int( (2**level)/2 )] = p**2\r\n",
        "\t  J = np.argsort(leaves)[:k]\r\n",
        "\t  I = []\r\n",
        "\t  for j in J:\r\n",
        "\t\t  bin_str = bin(j)[2:].zfill(int(np.log2(N)))\r\n",
        "\t\t  bin_str = bin_str[::-1]\r\n",
        "\t\t  bin_str = '0b' + bin_str\r\n",
        "\t\t  I.append(int(bin_str,2))\r\n",
        "\t  return sorted(I)\r\n",
        "\r\n",
        "def _polar_encode(u):\r\n",
        "\r\n",
        "    N = len(u)\r\n",
        "    n = 1\r\n",
        "    x = np.copy(u)\r\n",
        "    stages = np.log2(N).astype(int)\r\n",
        "    for s in range(0,stages):\r\n",
        "        i = 0\r\n",
        "        while i < N:\r\n",
        "            for j in range(0,n):\r\n",
        "                idx = i+j\r\n",
        "                x[idx] = x[idx] ^ x[idx+n]\r\n",
        "            i=i+2*n\r\n",
        "        n=2*n\r\n",
        "    return x\r\n",
        "\r\n",
        "def polar_encode(input, N, k, snr_db):\r\n",
        "    indices = get_index_set(N, k ,snr_db)\r\n",
        "    X = np.zeros((len(input), N),dtype=bool)\r\n",
        "    X[:, indices] = input\r\n",
        "    encoded = []\r\n",
        "\r\n",
        "    for u in X:\r\n",
        "        encoded.append(_polar_encode(u))\r\n",
        "    return encoded\r\n",
        "\r\n",
        "def BER_MAP_PolarCode(snr_db, val_size, N, k):\r\n",
        "    \r\n",
        "    flips = 0.0\r\n",
        "    total_bits = 0.0\r\n",
        "    for _ in range(10):\r\n",
        "\r\n",
        "        u_messages = np.random.randint(0, 2, size=(val_size, k))\r\n",
        "        for u in u_messages:\r\n",
        "            polar_code = PolarCode(N, k)\r\n",
        "            polar_code.construction_type = 'bb'\r\n",
        "            Construct(polar_code, snr_db)\r\n",
        "\r\n",
        "            polar_code.set_message(u)\r\n",
        "            \r\n",
        "            Encode(polar_code)\r\n",
        "            AWGN(polar_code, snr_db)\r\n",
        "            Decode(polar_code)\r\n",
        "\r\n",
        "            flips += np.sum(np.abs(u-polar_code.message_received))\r\n",
        "        total_bits += val_size*k\r\n",
        "    \r\n",
        "    return flips/(total_bits)\r\n",
        "    "
      ],
      "outputs": [],
      "metadata": {
        "id": "2WAamPcIr9pB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "k = 8\r\n",
        "N = 16"
      ],
      "outputs": [],
      "metadata": {
        "id": "YfKhfZQlrYNV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "#模拟信道损耗\r\n",
        "def bpsk_modulation(x):\r\n",
        "    return -2*x + 1\r\n",
        "    \r\n",
        "def awgn_noise(x, noise_var):\r\n",
        "    return x + K.random_normal(K.shape(x), mean = 0.0, stddev = np.sqrt(noise_var))\r\n",
        "    \r\n",
        "# def llr(x, noise_var):\r\n",
        "#     return 2*x/noise_var"
      ],
      "outputs": [],
      "metadata": {
        "id": "PDm3P7MDKxXb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "#生成数据\r\n",
        "NVE_list = []\r\n",
        "#u_messages：未编码：k位\r\n",
        "u_messages = np.array(generate_input_data(k), dtype = np.float32)\r\n",
        "print(len(u_messages))\r\n",
        "#x:编好码 N位\r\n",
        "x = np.array(polar_encode(u_messages, N, k, -3), np.float32)\r\n",
        "bpsk=x\r\n",
        "print(x)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "256\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 0. 1. ... 0. 1. 0.]\n",
            " ...\n",
            " [1. 1. 0. ... 0. 1. 1.]\n",
            " [1. 0. 0. ... 1. 1. 0.]\n",
            " [0. 1. 1. ... 0. 0. 1.]]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62nsAYjSrddi",
        "outputId": "eacb8bdb-d50a-4e87-9da6-74dead862fd2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "#增加数据数量至1024个码\r\n",
        "\r\n",
        "print(u_messages)\r\n",
        "# noise_var = 1.0/(2* 10**( (0 + 10*np.log10(0.01) )/10.0) )\r\n",
        "SNR = 10.0   #Eb/N0 单位dB\r\n",
        "noise_var = 10**(-SNR/(20*(k/N)))\r\n",
        "x=np.concatenate((x,x))\r\n",
        "x=np.concatenate((x,x))\r\n",
        "x=np.concatenate((x,x))\r\n",
        "x=np.concatenate((x,x))\r\n",
        "x=np.concatenate((x,x))\r\n",
        "# x=np.concatenate((x,x))\r\n",
        "# x=np.concatenate((x,x))\r\n",
        "# x=np.concatenate((x,x))\r\n",
        "\r\n",
        "x=x[:8192]\r\n",
        "# for a in x:\r\n",
        "#   print(a)\r\n",
        "#对数据进行信道损耗模拟\r\n",
        "x = bpsk_modulation(x)\r\n",
        "print(x[0])\r\n",
        "bpsk=bpsk_modulation(bpsk)\r\n",
        "x = awgn_noise(x, noise_var)\r\n",
        "x=np.around(x,1)\r\n",
        "print(len(u_messages))\r\n",
        "\r\n",
        "print(x[0])\r\n",
        "print(len(x))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " ...\n",
            " [1. 1. 1. ... 1. 0. 1.]\n",
            " [1. 1. 1. ... 1. 1. 0.]\n",
            " [1. 1. 1. ... 1. 1. 1.]]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "256\n",
            "[1.4 1.3 1.1 1.5 0.8 1.  1.  0.6 1.1 0.4 1.  1.  0.9 1.5 0.9 1.2]\n",
            "8192\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6U6CLijMhkJ",
        "outputId": "020d0836-559a-4a14-87c1-a641bc30ac98"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "max=0\r\n",
        "min=10\r\n",
        "for i in range(len(x)):\r\n",
        "  for j in range(16):\r\n",
        "    if x[i][j]>max:\r\n",
        "      max=x[i][j]\r\n",
        "    if x[i][j]<min:\r\n",
        "      min=x[i][j]\r\n",
        "print(min)\r\n",
        "print(max)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-2.4\n",
            "2.3\n"
          ]
        }
      ],
      "metadata": {
        "id": "aKWh-ru_Qc7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17d1dfe1-f220-442d-d9b4-081913306ed8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#算欧氏距离,改变误差过大数据\r\n",
        "#measure the distance of codes added with noise and code after bpsk\r\n",
        "x_b=[]\r\n",
        "u_m=[]\r\n",
        "for i in range(len(x)):\r\n",
        "  min=100000\r\n",
        "  for j in range(len(bpsk)):\r\n",
        "    dis=0\r\n",
        "    for l in range(N):\r\n",
        "      dis=dis+(bpsk[j][l]-x[i][l])**2\r\n",
        "    if dis<min:\r\n",
        "      min=dis\r\n",
        "      x_match_b=bpsk[j]\r\n",
        "      ind=j\r\n",
        "  x_b.append(x_match_b)\r\n",
        "  u_m.append(u_messages[ind])\r\n",
        "\r\n",
        "print(x[0])\r\n",
        "print(bpsk[0])\r\n",
        "print(x_b[0]) \r\n",
        "print(u_m[0])\r\n",
        "u_messages=u_m\r\n",
        "print(u_messages)\r\n",
        "print(len(u_messages))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEm_DGaaiAyj",
        "outputId": "42fb30e5-d4db-4e92-fbc8-efb09599302b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "source": [
        "#将数据导入code.csv\r\n",
        "#input the data\r\n",
        "import csv\r\n",
        "writer=csv.writer(open('./code.csv','w'))\r\n",
        "length_list=len(u_messages)\r\n",
        "i=0\r\n",
        "while i!=length_list+1 :\r\n",
        "    if(i==0):\r\n",
        "      data='code'\r\n",
        "      data2='encode'\r\n",
        "    else:\r\n",
        "      # print(i)\r\n",
        "      data=u_messages[i-1]\r\n",
        "      data2=x[i-1]\r\n",
        "    #print data\r\n",
        "    i=i+1\r\n",
        "    writer.writerow([data,data2])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# city = pd.DataFrame([u_messages[0],1])\r\n",
        "# city.to_csv('./code.csv')"
      ],
      "outputs": [],
      "metadata": {
        "id": "pBXtDjtqA-oW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df = pd.read_csv(\"./code.csv\", delimiter=',', header=None, names=['code','encode'])\r\n",
        "df.sample(10)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "-yTRrJbhL-uk",
        "outputId": "f510d3e1-d6a9-475a-8506-5dfe7828e342"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "source": [
        "\r\n",
        "code=df.code.values\r\n",
        "encode=df.encode.values\r\n",
        "# #encode=encode[1:854]\r\n",
        "code=code[1:]\r\n",
        "encode=encode[1:]\r\n",
        "print(code[-1])\r\n",
        "print(x[-1])\r\n",
        "print(encode[-1])\r\n",
        "# print(encode[0:10])\r\n",
        "# # xx=df.label.values\r\n",
        "# # print(xx)\r\n",
        "# print(len(encode))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "[ 1.3 -1.3 -1.3 -0.8 -0.1 -0.8 -0.6  1.1 -1.   0.5  1.   0.8  1.5  0.9\n",
            "  1.3 -0.7]\n",
            "[ 1.3 -1.3 -1.3 -0.8 -0.1 -0.8 -0.6  1.1 -1.   0.5  1.   0.8  1.5  0.9\n",
            "  1.3 -0.7]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdEqlIRxgIAl",
        "outputId": "d05ad2a7-d13b-4f02-9780-72dce9c8e15e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "source": [
        "import torch"
      ],
      "outputs": [],
      "metadata": {
        "id": "4b0ZcRP9kLdG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "#保留一位小数\r\n",
        "#snr=3 -4.3-4.3\r\n",
        "#snr=10 -2.4-2.4  \r\n",
        "#snr=9 -2.5-2,5 \r\n",
        "a=np.zeros(51)\r\n",
        "for i in range(51):\r\n",
        "  a[i]=-2.5+i*0.1\r\n",
        "\r\n",
        "np.round(a,1)\r\n",
        "s=[]\r\n",
        "#print(a)\r\n",
        "for i in range(51):\r\n",
        "  st=str(a[i])\r\n",
        "  if st[0]=='-':\r\n",
        "    if len(st)>5:\r\n",
        "      if st[4]=='0':\r\n",
        "        st=st[0:4]\r\n",
        "      else:\r\n",
        "        if st[3]=='9':\r\n",
        "              t=int(st[1])\r\n",
        "              t=t+1\r\n",
        "              st=st[0]\r\n",
        "              st=st+str(t)\r\n",
        "              st=st+'.'\r\n",
        "        else:\r\n",
        "              t=int(st[3])\r\n",
        "              t=t+1\r\n",
        "              st=st[0:3]\r\n",
        "              st=st+str(t)\r\n",
        "        # else:\r\n",
        "        #   g=int(st[4])\r\n",
        "        #   g=g+1\r\n",
        "        #   st=st[0:4]############\r\n",
        "        #   st=st+str(g)\r\n",
        "  else:\r\n",
        "    if len(st)>5:\r\n",
        "      if st[3]=='0':\r\n",
        "        st=st[0:3]\r\n",
        "      else:\r\n",
        "        if st[2]=='9':\r\n",
        "            \r\n",
        "              t=int(st[0])\r\n",
        "              t=t+1\r\n",
        "              st=''\r\n",
        "              st=st+str(t)\r\n",
        "              st=st+'.'\r\n",
        "        else:\r\n",
        "              t=int(st[2])\r\n",
        "              t=t+1\r\n",
        "              st=st[0:2]\r\n",
        "              st=st+str(t)\r\n",
        "        # else:\r\n",
        "        #   g=int(st[3])\r\n",
        "        #   g=g+1\r\n",
        "        #   st=st[0:3]############\r\n",
        "        #   st=st+str(g)\r\n",
        "\r\n",
        "    \r\n",
        "  if(st[-1]=='0'):\r\n",
        "    st=st[:-1]\r\n",
        "    if(st[-1]=='0'):\r\n",
        "      st=st[:-1]\r\n",
        "\r\n",
        "  s.append(st)\r\n",
        "print(s)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['-2.5', '-2.4', '-2.3', '-2.2', '-2.1', '-2.', '-1.9', '-1.8', '-1.7', '-1.6', '-1.5', '-1.4', '-1.3', '-1.2', '-1.1', '-1.', '-0.9', '-0.8', '-0.7', '-0.6', '-0.5', '-0.4', '-0.3', '-0.2', '-0.1', '0.', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '1.', '1.1', '1.2', '1.3', '1.4', '1.5', '1.6', '1.7', '1.8', '1.9', '2.', '2.1', '2.2', '2.3', '2.4', '2.5']\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6BxnKk3wojE",
        "outputId": "14bd82bc-f107-4843-f996-e836a97136e2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "source": [
        "##分词，构建输入输出词表\r\n",
        "# S: Symbol that shows starting of decoding input\r\n",
        "# E: Symbol that shows starting of decoding output\r\n",
        "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\r\n",
        "import math\r\n",
        "import torch\r\n",
        "import numpy as np\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.utils.data as Data\r\n",
        "import re\r\n",
        "# Padding Should be Zero\r\n",
        "src_vocab = {'P' : 0}\r\n",
        "i=0\r\n",
        "for i in range(len(s)):\r\n",
        "  src_vocab[s[i]]=i+1\r\n",
        "src_vocab['-0.']=26\r\n",
        "# for sent in encode:\r\n",
        "#   sent=sent[1:-1]\r\n",
        "#   sent=sent.split()\r\n",
        "#   for s in sent:\r\n",
        "#     if(s in src_vocab.keys()):\r\n",
        "#       continue\r\n",
        "#     src_vocab[s]=i+1\r\n",
        "#     i=i+1\r\n",
        "\r\n",
        "# print(len(encode))\r\n",
        "# for i in range(len(encode)):\r\n",
        "# for j in range(len(encode[0])):\r\n",
        "#   print(encode[i][j])\r\n",
        "#     # if(encode[i][j] in src_vocab.keys()):\r\n",
        "#     #   continue\r\n",
        "#     # src_vocab[encode[i][j]]=i*8+j+1\r\n",
        "print(src_vocab)\r\n",
        "src_vocab_size = len(src_vocab)\r\n",
        "batch_size=32\r\n",
        "tgt_vocab = {'P' : 0, '0.' : 1, '1.' : 2, 'S' : 3, 'E' : 4}\r\n",
        "idx2word = {i: w for i, w in enumerate(tgt_vocab)}\r\n",
        "tgt_vocab_size = len(tgt_vocab)\r\n",
        "\r\n",
        "src_len = 16 # enc_input max sequence length\r\n",
        "tgt_len = 8 # dec_input(=dec_output) max sequence length\r\n",
        "# Transformer Parameters\r\n",
        "d_model = 512  # Embedding Size\r\n",
        "d_ff = 2048 # FeedForward dimension\r\n",
        "d_k = d_v = 64  # dimension of K(=Q), V\r\n",
        "\r\n",
        "n_layers = 6  # number of Encoder of Decoder Layer\r\n",
        "n_heads = 8  # number of heads in Multi-Head Attention"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'P': 0, '-2.5': 1, '-2.4': 2, '-2.3': 3, '-2.2': 4, '-2.1': 5, '-2.': 6, '-1.9': 7, '-1.8': 8, '-1.7': 9, '-1.6': 10, '-1.5': 11, '-1.4': 12, '-1.3': 13, '-1.2': 14, '-1.1': 15, '-1.': 16, '-0.9': 17, '-0.8': 18, '-0.7': 19, '-0.6': 20, '-0.5': 21, '-0.4': 22, '-0.3': 23, '-0.2': 24, '-0.1': 25, '0.': 26, '0.1': 27, '0.2': 28, '0.3': 29, '0.4': 30, '0.5': 31, '0.6': 32, '0.7': 33, '0.8': 34, '0.9': 35, '1.': 36, '1.1': 37, '1.2': 38, '1.3': 39, '1.4': 40, '1.5': 41, '1.6': 42, '1.7': 43, '1.8': 44, '1.9': 45, '2.': 46, '2.1': 47, '2.2': 48, '2.3': 49, '2.4': 50, '2.5': 51, '-0.': 26}\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYPPsorUf8pN",
        "outputId": "2b7cbe5e-e2cc-413e-91f9-2ecc11b78d8c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def make_data(encode,code):#数据预处理\r\n",
        "    enc_inputs, dec_inputs, dec_outputs = [], [], []\r\n",
        "    for i in range(len(encode)):\r\n",
        "      en=encode[i][1:-1].split()\r\n",
        "      co=code[i][1:-1].split()\r\n",
        "      #print(co)\r\n",
        "      enc_input = [[src_vocab[n] for n in en]] # [[1, 2, 3, 4, 0], [1, 2, 3, 5, 0]]\r\n",
        "      dec_input=[3]\r\n",
        "      dec_output=[]\r\n",
        "\r\n",
        "      for n in co:\r\n",
        "        dec_input.append(tgt_vocab[n])\r\n",
        "        dec_output.append(tgt_vocab[n])\r\n",
        "       # [[6, 1, 2, 3, 4, 8], [6, 1, 2, 3, 5, 8]]\r\n",
        "       # [[1, 2, 3, 4, 8, 7], [1, 2, 3, 5, 8, 7]]\r\n",
        "      # print(dec_input)\r\n",
        "      \r\n",
        "      dec_output.append(4)\r\n",
        "      dec_output=[dec_output]\r\n",
        "      dec_input=[dec_input]\r\n",
        "      enc_inputs.extend(enc_input)\r\n",
        "      dec_inputs.extend(dec_input)\r\n",
        "      dec_outputs.extend(dec_output)\r\n",
        "      \r\n",
        "    print(enc_inputs)\r\n",
        "    print(dec_inputs)\r\n",
        "    print(dec_outputs)\r\n",
        "    return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)\r\n",
        "    #\r\n",
        "\r\n",
        "enc_inputs, dec_inputs, dec_outputs = make_data(encode,code)\r\n",
        "# \r\n",
        "class MyDataSet(Data.Dataset):\r\n",
        "  def __init__(self, enc_inputs, dec_inputs, dec_outputs):\r\n",
        "    super(MyDataSet, self).__init__()\r\n",
        "    self.enc_inputs = enc_inputs\r\n",
        "    self.dec_inputs = dec_inputs\r\n",
        "    self.dec_outputs = dec_outputs\r\n",
        "  \r\n",
        "  def __len__(self):\r\n",
        "    return self.enc_inputs.shape[0]\r\n",
        "  \r\n",
        "  def __getitem__(self, idx):\r\n",
        "    return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\r\n",
        "\r\n",
        "a1=enc_inputs\r\n",
        "a2=dec_inputs\r\n",
        "a3=dec_outputs\r\n",
        "training_data=MyDataSet(a1,a2,a3)\r\n",
        "train_loader = Data.DataLoader(training_data,batch_size,True)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsQWy11vqEiE",
        "outputId": "9b3901f5-112b-4eb2-9e49-7374a6a170db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "构建transformer"
      ],
      "metadata": {
        "id": "ERzfa0Qvz-16"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "source": [
        "class PositionalEncoding(nn.Module):\r\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\r\n",
        "        super(PositionalEncoding, self).__init__()\r\n",
        "        self.dropout = nn.Dropout(p=dropout)\r\n",
        "\r\n",
        "        pe = torch.zeros(max_len, d_model)\r\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\r\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\r\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\r\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\r\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\r\n",
        "        self.register_buffer('pe', pe)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        '''\r\n",
        "        x: [seq_len, batch_size, d_model]\r\n",
        "        '''\r\n",
        "        x = x + self.pe[:x.size(0), :]\r\n",
        "        return self.dropout(x)\r\n",
        "\r\n",
        "def get_attn_pad_mask(seq_q, seq_k):\r\n",
        "    '''\r\n",
        "    seq_q: [batch_size, seq_len]\r\n",
        "    seq_k: [batch_size, seq_len]\r\n",
        "    seq_len could be src_len or it could be tgt_len\r\n",
        "    seq_len in seq_q and seq_len in seq_k maybe not equal\r\n",
        "    '''\r\n",
        "    batch_size, len_q = seq_q.size()\r\n",
        "    batch_size, len_k = seq_k.size()\r\n",
        "    # eq(zero) is PAD token\r\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # [batch_size, 1, len_k], False is masked\r\n",
        "    #print(pad_attn_mask)\r\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # [batch_size, len_q, len_k]\r\n",
        "\r\n",
        "def get_attn_subsequence_mask(seq):\r\n",
        "    '''\r\n",
        "    seq: [batch_size, tgt_len]\r\n",
        "    '''\r\n",
        "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\r\n",
        "    subsequence_mask = np.triu(np.ones(attn_shape), k=1) # Upper triangular matrix\r\n",
        "    subsequence_mask = torch.from_numpy(subsequence_mask).byte()\r\n",
        "    return subsequence_mask # [batch_size, tgt_len, tgt_len]"
      ],
      "outputs": [],
      "metadata": {
        "id": "dlWIEHkq0DAJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "source": [
        "class ScaledDotProductAttention(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(ScaledDotProductAttention, self).__init__()\r\n",
        "\r\n",
        "    def forward(self, Q, K, V, attn_mask):\r\n",
        "        '''\r\n",
        "        Q: [batch_size, n_heads, len_q, d_k]\r\n",
        "        K: [batch_size, n_heads, len_k, d_k]\r\n",
        "        V: [batch_size, n_heads, len_v(=len_k), d_v]\r\n",
        "        attn_mask: [batch_size, n_heads, seq_len, seq_len]\r\n",
        "        '''\r\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size, n_heads, len_q, len_k]\r\n",
        "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is True.\r\n",
        "        \r\n",
        "        attn = nn.Softmax(dim=-1)(scores)\r\n",
        "        context = torch.matmul(attn, V) # [batch_size, n_heads, len_q, d_v]\r\n",
        "        return context, attn\r\n",
        "\r\n",
        "class MultiHeadAttention(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(MultiHeadAttention, self).__init__()\r\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\r\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\r\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\r\n",
        "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\r\n",
        "    def forward(self, input_Q, input_K, input_V, attn_mask):\r\n",
        "        '''\r\n",
        "        input_Q: [batch_size, len_q, d_model]\r\n",
        "        input_K: [batch_size, len_k, d_model]\r\n",
        "        input_V: [batch_size, len_v(=len_k), d_model]\r\n",
        "        attn_mask: [batch_size, seq_len, seq_len]\r\n",
        "        '''\r\n",
        "        residual, batch_size = input_Q, input_Q.size(0)\r\n",
        "        # (B, S, D) -proj-> (B, S, D_new) -split-> (B, S, H, W) -trans-> (B, H, S, W)\r\n",
        "        Q = self.W_Q(input_Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # Q: [batch_size, n_heads, len_q, d_k]\r\n",
        "        K = self.W_K(input_K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # K: [batch_size, n_heads, len_k, d_k]\r\n",
        "        V = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # V: [batch_size, n_heads, len_v(=len_k), d_v]\r\n",
        "\r\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size, n_heads, seq_len, seq_len]\r\n",
        "\r\n",
        "        # context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]\r\n",
        "        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)\r\n",
        "        context = context.transpose(1, 2).reshape(batch_size, -1, n_heads * d_v) # context: [batch_size, len_q, n_heads * d_v]\r\n",
        "        output = self.fc(context) # [batch_size, len_q, d_model]\r\n",
        "        return nn.LayerNorm(d_model).cuda()(output + residual), attn\r\n",
        "\r\n",
        "class PoswiseFeedForwardNet(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\r\n",
        "        self.fc = nn.Sequential(\r\n",
        "            nn.Linear(d_model, d_ff, bias=False),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Linear(d_ff, d_model, bias=False)\r\n",
        "        )\r\n",
        "    def forward(self, inputs):\r\n",
        "        '''\r\n",
        "        inputs: [batch_size, seq_len, d_model]\r\n",
        "        '''\r\n",
        "        residual = inputs\r\n",
        "        output = self.fc(inputs)\r\n",
        "        return nn.LayerNorm(d_model).cuda()(output + residual) # [batch_size, seq_len, d_model]\r\n",
        "\r\n",
        "class EncoderLayer(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(EncoderLayer, self).__init__()\r\n",
        "        # self.enc_self_attn = MultiHeadAttention()\r\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\r\n",
        "\r\n",
        "    def forward(self, enc_inputs):\r\n",
        "        '''\r\n",
        "        enc_inputs: [batch_size, src_len, d_model]\r\n",
        "        enc_self_attn_mask: [batch_size, src_len, src_len]\r\n",
        "        '''\r\n",
        "        # enc_outputs: [batch_size, src_len, d_model], attn: [batch_size, n_heads, src_len, src_len]\r\n",
        "        #enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\r\n",
        "        enc_outputs = self.pos_ffn(enc_inputs) # enc_outputs: [batch_size, src_len, d_model]\r\n",
        "        return enc_outputs\r\n",
        "\r\n",
        "class DecoderLayer(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(DecoderLayer, self).__init__()\r\n",
        "        self.dec_self_attn = MultiHeadAttention()\r\n",
        "        self.dec_enc_attn = MultiHeadAttention()\r\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\r\n",
        "\r\n",
        "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\r\n",
        "        '''\r\n",
        "        dec_inputs: [batch_size, tgt_len, d_model]\r\n",
        "        enc_outputs: [batch_size, src_len, d_model]\r\n",
        "        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\r\n",
        "        dec_enc_attn_mask: [batch_size, tgt_len, src_len]\r\n",
        "        '''\r\n",
        "        # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]\r\n",
        "        #经过一次self-attention\r\n",
        "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\r\n",
        "\r\n",
        "        # dec_outputs: [batch_size, tgt_len, d_model], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\r\n",
        "        #将self-attention的output作为q,encoder的output作为k.v的输入进行再一次attention\r\n",
        "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\r\n",
        "        dec_outputs = self.pos_ffn(dec_outputs) # [batch_size, tgt_len, d_model]\r\n",
        "        return dec_outputs, dec_self_attn, dec_enc_attn\r\n",
        "\r\n",
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Encoder, self).__init__()\r\n",
        "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\r\n",
        "        self.pos_emb = PositionalEncoding(d_model)\r\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\r\n",
        "\r\n",
        "    def forward(self, enc_inputs):\r\n",
        "        '''\r\n",
        "        enc_inputs: [batch_size, src_len]\r\n",
        "        '''\r\n",
        "        enc_outputs = self.src_emb(enc_inputs) # [batch_size, src_len, d_model]#################d_model为词向量长度\r\n",
        "        enc_outputs = self.pos_emb(enc_outputs.transpose(0, 1)).transpose(0, 1) # [batch_size, src_len, d_model]\r\n",
        "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs) # [batch_size, src_len, src_len]\r\n",
        "        #enc_self_attns = []\r\n",
        "        for layer in self.layers:\r\n",
        "            # enc_outputs: [batch_size, src_len, d_model], enc_self_attn: [batch_size, n_heads, src_len, src_len]\r\n",
        "            enc_outputs = layer(enc_outputs)\r\n",
        "            #enc_self_attns.append(enc_self_attn)\r\n",
        "        #print(enc_self_attns)#############1,一个疑问，这是什么\r\n",
        "        return enc_outputs\r\n",
        "\r\n",
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Decoder, self).__init__()\r\n",
        "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\r\n",
        "        self.pos_emb = PositionalEncoding(d_model)\r\n",
        "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\r\n",
        "\r\n",
        "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\r\n",
        "        '''\r\n",
        "        dec_inputs: [batch_size, tgt_len]\r\n",
        "        enc_inputs: [batch_size, src_len]\r\n",
        "        enc_outputs: [batsh_size, src_len, d_model]\r\n",
        "        '''\r\n",
        "        #给单词编码\r\n",
        "        dec_outputs = self.tgt_emb(dec_inputs) # [batch_size, tgt_len, d_model]\r\n",
        "        ####################问题2，为什么要用cuda?\r\n",
        "        dec_outputs = self.pos_emb(dec_outputs.transpose(0, 1)).transpose(0, 1).cuda() # [batch_size, tgt_len, d_model]\r\n",
        "\r\n",
        "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).cuda() # [batch_size, tgt_len, tgt_len]\r\n",
        "        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).cuda() # [batch_size, tgt_len, tgt_len]#####一个主要作用是屏蔽未来时刻单词的信息。\r\n",
        "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask), 0).cuda() # [batch_size, tgt_len, tgt_len]【true false\r\n",
        "        #以上几行mask掉了未来时刻和pad的信息\r\n",
        "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs) # [batc_size, tgt_len, src_len]#问题四，为啥mask是这个\r\n",
        "        #tensor([[[False, False, False, False,  True],\r\n",
        "        #  [False, False, False, False,  True],\r\n",
        "        #  [False, False, False, False,  True],\r\n",
        "        #  [False, False, False, False,  True],\r\n",
        "        #  [False, False, False, False,  True],\r\n",
        "        #  [False, False, False, False,  True]],\r\n",
        "\r\n",
        "        # [[False, False, False, False,  True],\r\n",
        "        #  [False, False, False, False,  True],\r\n",
        "        #  [False, False, False, False,  True],\r\n",
        "        #  [False, False, False, False,  True],\r\n",
        "        #  [False, False, False, False,  True],\r\n",
        "        #  [False, False, False, False,  True]]])\r\n",
        "\r\n",
        "        dec_self_attns, dec_enc_attns = [], []\r\n",
        "        for layer in self.layers:\r\n",
        "            # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\r\n",
        "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)#######问题3这步里面是啥\r\n",
        "            dec_self_attns.append(dec_self_attn)\r\n",
        "            dec_enc_attns.append(dec_enc_attn)\r\n",
        "        return dec_outputs, dec_self_attns, dec_enc_attns\r\n",
        "\r\n",
        "class Transformer(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Transformer, self).__init__()\r\n",
        "        self.encoder = Encoder().cuda()\r\n",
        "        self.decoder = Decoder().cuda()\r\n",
        "        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False).cuda()\r\n",
        "    def forward(self, enc_inputs, dec_inputs):\r\n",
        "        '''\r\n",
        "        enc_inputs: [batch_size, src_len]\r\n",
        "        dec_inputs: [batch_size, tgt_len]\r\n",
        "        '''\r\n",
        "        # tensor to store decoder outputs\r\n",
        "        # outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\r\n",
        "        \r\n",
        "        # enc_outputs: [batch_size, src_len, d_model], enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]\r\n",
        "        enc_outputs= self.encoder(enc_inputs)\r\n",
        "        # dec_outpus: [batch_size, tgt_len, d_model], dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]\r\n",
        "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\r\n",
        "        dec_logits = self.projection(dec_outputs) # dec_logits: [batch_size, tgt_len, tgt_vocab_size]\r\n",
        "        #print(dec_logits)\r\n",
        "        return dec_logits.view(-1, dec_logits.size(-1)),  dec_self_attns, dec_enc_attns\r\n",
        "\r\n",
        "model = Transformer().cuda()\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\r\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\r\n",
        "#optimizer = optim.Adam(model.parameters(), lr=1e-2)\r\n",
        "# ckpt=torch.load(\"para.pth\")\r\n",
        "# model.load_state_dict(ckpt)\r\n",
        "loss_list=[]\r\n",
        "xx=[]\r\n",
        "min_loss=1e9"
      ],
      "outputs": [],
      "metadata": {
        "id": "kax-QdvK0PCZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\r\n",
        "\r\n",
        "for epoch in range(200):\r\n",
        "  \r\n",
        "    for enc_inputs, dec_inputs, dec_outputs in train_loader:\r\n",
        "      '''\r\n",
        "      enc_inputs: [batch_size, src_len]\r\n",
        "      dec_inputs: [batch_size, tgt_len]\r\n",
        "      dec_outputs: [batch_size, tgt_len]\r\n",
        "      '''\r\n",
        "      enc_inputs, dec_inputs, dec_outputs = enc_inputs.cuda(), dec_inputs.cuda(), dec_outputs.cuda()\r\n",
        "      # outputs: [batch_size * tgt_len, tgt_vocab_size]\r\n",
        "      outputs, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\r\n",
        "      loss = criterion(outputs, dec_outputs.view(-1))\r\n",
        "      \r\n",
        "      optimizer.zero_grad()\r\n",
        "      loss.backward()\r\n",
        "      optimizer.step()\r\n",
        "    if loss<min_loss:\r\n",
        "      torch.save(model.state_dict(),\"para2.pth\")\r\n",
        "    loss_list.append(loss)\r\n",
        "    xx.append(epoch)\r\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\r\n",
        "    # scheduler.step()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss = 0.615096\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh8S1RRm08uY",
        "outputId": "85cba473-7fe7-4228-9cf4-cd401ce306a3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\r\n",
        "\r\n",
        "for i in range(len(xx)):\r\n",
        "  xx[i]=i\r\n",
        "# for i in range(120,150):\r\n",
        "#   xx[i]=xx[i]+120\r\n",
        "# # for i in range(20,60):\r\n",
        "# #   xx[i]=xx[i]-40\r\n",
        "# # for i in range(60,110):\r\n",
        "# #   xx[i]=xx[i]+100\r\n",
        "print(xx)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKYCwfc9VlXN",
        "outputId": "232154dd-c630-43f0-86cb-58b45e297d8f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\r\n",
        "plt.figure()\r\n",
        "plt.plot(xx,loss_list)\r\n",
        "plt.xlabel('epoch') \r\n",
        "plt.ylabel('loss')\r\n",
        "plt.show()\r\n",
        "l=[]\r\n",
        "for i in range(len(loss_list)):\r\n",
        "  l.append(float(loss_list[i].data))\r\n",
        "with open(\"loss9.txt\",\"w\")as f:\r\n",
        "  print(l,file=f)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "K5E7hXQ_OV4m",
        "outputId": "3b8f23f9-b712-4552-a77d-cf1e16ca251a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#误码率\r\n",
        "\r\n",
        "torch.save(model.state_dict(),\"para.pth\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "DtdYDvOnI6A4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "_qs-gT-vokz2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def greedy_decoder(model, enc_input, start_symbol):\r\n",
        "    \"\"\"\r\n",
        "    For simplicity, a Greedy Decoder is Beam search when K=1. This is necessary for inference as we don't know the\r\n",
        "    target sequence input. Therefore we try to generate the target input word by word, then feed it into the transformer.\r\n",
        "    Starting Reference: http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding\r\n",
        "    :param model: Transformer Model\r\n",
        "    :param enc_input: The encoder input\r\n",
        "    :param start_symbol: The start symbol. In this example it is 'S' which corresponds to index 4\r\n",
        "    :return: The target input\r\n",
        "    \"\"\"\r\n",
        "    enc_input=enc_input.cuda()\r\n",
        "    enc_outputs = model.encoder(enc_input)\r\n",
        "    dec_input = torch.zeros(1, 0).type_as(enc_input.data)\r\n",
        "    dec_input=dec_input.cuda()\r\n",
        "    terminal = False\r\n",
        "    next_symbol = start_symbol\r\n",
        "    for i in range(8):  \r\n",
        "        a= torch.tensor([[next_symbol]],dtype=enc_input.dtype)\r\n",
        "        a=a.cuda()      \r\n",
        "        dec_input=torch.cat([dec_input.detach(),a],-1)\r\n",
        "        dec_input=dec_input.cuda()\r\n",
        "        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\r\n",
        "        projected = model.projection(dec_outputs)\r\n",
        "        prob = projected.squeeze(0).max(dim=-1, keepdim=False)[1]\r\n",
        "        next_word = prob.data[-1]\r\n",
        "        next_symbol = next_word\r\n",
        "        #print(next_word)            \r\n",
        "    return dec_input"
      ],
      "outputs": [],
      "metadata": {
        "id": "54eszBfj2ckk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "test_u = np.array(np.random.randint(0, 2, size = (5000, k)), dtype = np.float32)\r\n",
        "test_x = np.array(polar_encode(test_u, N, k, -3), np.float32)\r\n",
        "test_x = bpsk_modulation(test_x)\r\n",
        "#print(test_x[0])\r\n",
        "test_x = awgn_noise(test_x, noise_var)\r\n",
        "test_x=np.around(test_x,1)\r\n",
        "print(SNR)\r\n",
        "print(test_x)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.0\n",
            "[[ 0.79  1.2  -1.11 ...  0.8  -0.83 -1.3 ]\n",
            " [-0.83 -1.37 -1.04 ...  0.87  0.96 -1.68]\n",
            " [-0.96  1.12  1.18 ... -1.   -1.76 -0.68]\n",
            " ...\n",
            " [ 1.75  0.56  0.94 ... -0.91  0.62 -0.73]\n",
            " [-0.77 -0.69  1.25 ...  0.49 -1.88  0.96]\n",
            " [-0.68  1.28  0.62 ...  0.51  0.99 -1.35]]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIwyn0ja9U1J",
        "outputId": "1cdc85f7-f926-4105-e25b-7f1b0d24c3c1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(test_u)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 1. 0. ... 0. 0. 1.]\n",
            " [0. 1. 1. ... 0. 0. 1.]\n",
            " [0. 1. 1. ... 0. 1. 1.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 1. 0. ... 0. 0. 1.]\n",
            " [0. 0. 1. ... 1. 1. 1.]]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69RTAVHpOSia",
        "outputId": "907b5b1c-0acf-4c67-f532-a10cfde783cb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#将数据导入code.csv\r\n",
        "writer=csv.writer(open('./testcode.csv','w'))\r\n",
        "length_list=len(test_u)\r\n",
        "i=0\r\n",
        "while i!=length_list+1 :\r\n",
        "    if(i==0):\r\n",
        "      data='code'\r\n",
        "      data2='encode'\r\n",
        "    else:\r\n",
        "      # print(i)\r\n",
        "      data=test_u[i-1]\r\n",
        "      data2=test_x[i-1]\r\n",
        "    #print data\r\n",
        "    i=i+1\r\n",
        "    writer.writerow([data,data2])\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "euSeeb8QLmc9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df = pd.read_csv(\"./testcode.csv\", delimiter=',', header=None, names=['code','encode'])\r\n",
        "\r\n",
        "answer=df.code.values\r\n",
        "ques=df.encode.values\r\n",
        "# #encode=encode[1:854]\r\n",
        "answer=answer[1:]\r\n",
        "ques=ques[1:]"
      ],
      "outputs": [],
      "metadata": {
        "id": "BixDpUsOMFYM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "a, b, c = make_data(ques,answer)\r\n",
        "test_data=MyDataSet(a,b,c)\r\n",
        "test_loader=Data.DataLoader(test_data,batch_size,False)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5Y6szvcMtPE",
        "outputId": "9c0ffd17-26d0-4afc-c59b-814b06b38850"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "  #Test\r\n",
        "acc=0\r\n",
        "cnt=0\r\n",
        "for enc_inputs, _, label in test_loader:\r\n",
        "  #print(enc_inputs[:20])\r\n",
        "  #print(label[:20])\r\n",
        "  #print(label)\r\n",
        "  #print(o\r\n",
        "    correct=0\r\n",
        "    total=0\r\n",
        "    enc_inputs=enc_inputs.cuda()\r\n",
        "    #print(len(enc_inputs))\r\n",
        "    #print(src_vocab)\r\n",
        "    for i in range(len(enc_inputs)):\r\n",
        "        greedy_dec_input = greedy_decoder(model, enc_inputs[i].view(1, -1), start_symbol=tgt_vocab[\"S\"])\r\n",
        "        predict, _, _ = model(enc_inputs[i].view(1, -1), greedy_dec_input)\r\n",
        "        #print(predict)\r\n",
        "        a=label[i][:-1]\r\n",
        "        for j in range(len(a)):\r\n",
        "          a[j]=a[j]-1\r\n",
        "        \r\n",
        "        \r\n",
        "        predict = predict.data.max(1, keepdim=True)[1]\r\n",
        "        b=[idx2word[n.item()] for n in predict.squeeze()]\r\n",
        "        for j in range(8):\r\n",
        "          #print(b[j],a[j])\r\n",
        "          #print(type(a[j]))\r\n",
        "          c=1\r\n",
        "          if b[j]=='0.':\r\n",
        "            c=0\r\n",
        "          if c==a[j]:  \r\n",
        "            correct=correct+1\r\n",
        "          if c!=a[j]:\r\n",
        "            cnt=cnt+1\r\n",
        "            print(\"real\",a)\r\n",
        "            print(enc_inputs[i], '->', [idx2word[n.item()] for n in predict.squeeze()])\r\n",
        "          total=total+1\r\n",
        "        \r\n",
        "print(\"ber=\",cnt/40000)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "real tensor([0, 1, 1, 0, 1, 1, 0, 0])\n",
            "tensor([331, 352, 187, 329, 338, 109, 314, 358, 308, 311, 261, 378, 349, 117,\n",
            "        343, 342], device='cuda:0') -> ['0.', '1.', '0.', '0.', '1.', '1.', '0.', '0.']\n",
            "real tensor([1, 0, 0, 1, 1, 1, 1, 0])\n",
            "tensor([129, 160, 150, 396, 295, 337, 331, 177, 352, 320, 226, 161, 113, 173,\n",
            "        124, 374], device='cuda:0') -> ['1.', '0.', '1.', '1.', '1.', '1.', '1.', '0.']\n",
            "real tensor([0, 0, 0, 1, 0, 0, 1, 0])\n",
            "tensor([327, 139, 319, 126, 182, 314, 136, 354, 331, 138, 347, 208, 160, 407,\n",
            "        215, 344], device='cuda:0') -> ['0.', '0.', '0.', '1.', '0.', '1.', '0.', '1.']\n",
            "real tensor([0, 0, 0, 1, 0, 0, 1, 0])\n",
            "tensor([327, 139, 319, 126, 182, 314, 136, 354, 331, 138, 347, 208, 160, 407,\n",
            "        215, 344], device='cuda:0') -> ['0.', '0.', '0.', '1.', '0.', '1.', '0.', '1.']\n",
            "real tensor([0, 0, 0, 1, 0, 0, 1, 0])\n",
            "tensor([327, 139, 319, 126, 182, 314, 136, 354, 331, 138, 347, 208, 160, 407,\n",
            "        215, 344], device='cuda:0') -> ['0.', '0.', '0.', '1.', '0.', '1.', '0.', '1.']\n",
            "real tensor([0, 0, 1, 1, 1, 1, 0, 1])\n",
            "tensor([130, 128, 153, 368, 178, 376, 225, 110, 128, 100, 174, 420, 158, 342,\n",
            "        127, 245], device='cuda:0') -> ['0.', '1.', '0.', '0.', '0.', '0.', '1.', '0.']\n",
            "real tensor([0, 0, 1, 1, 1, 1, 0, 1])\n",
            "tensor([130, 128, 153, 368, 178, 376, 225, 110, 128, 100, 174, 420, 158, 342,\n",
            "        127, 245], device='cuda:0') -> ['0.', '1.', '0.', '0.', '0.', '0.', '1.', '0.']\n",
            "real tensor([0, 0, 1, 1, 1, 1, 0, 1])\n",
            "tensor([130, 128, 153, 368, 178, 376, 225, 110, 128, 100, 174, 420, 158, 342,\n",
            "        127, 245], device='cuda:0') -> ['0.', '1.', '0.', '0.', '0.', '0.', '1.', '0.']\n",
            "real tensor([0, 0, 1, 1, 1, 1, 0, 1])\n",
            "tensor([130, 128, 153, 368, 178, 376, 225, 110, 128, 100, 174, 420, 158, 342,\n",
            "        127, 245], device='cuda:0') -> ['0.', '1.', '0.', '0.', '0.', '0.', '1.', '0.']\n",
            "real tensor([0, 0, 1, 1, 1, 1, 0, 1])\n",
            "tensor([130, 128, 153, 368, 178, 376, 225, 110, 128, 100, 174, 420, 158, 342,\n",
            "        127, 245], device='cuda:0') -> ['0.', '1.', '0.', '0.', '0.', '0.', '1.', '0.']\n",
            "real tensor([0, 0, 1, 1, 1, 1, 0, 1])\n",
            "tensor([130, 128, 153, 368, 178, 376, 225, 110, 128, 100, 174, 420, 158, 342,\n",
            "        127, 245], device='cuda:0') -> ['0.', '1.', '0.', '0.', '0.', '0.', '1.', '0.']\n",
            "real tensor([0, 0, 1, 1, 1, 1, 0, 1])\n",
            "tensor([130, 128, 153, 368, 178, 376, 225, 110, 128, 100, 174, 420, 158, 342,\n",
            "        127, 245], device='cuda:0') -> ['0.', '1.', '0.', '0.', '0.', '0.', '1.', '0.']\n",
            "real tensor([0, 1, 1, 1, 0, 0, 0, 1])\n",
            "tensor([365, 172, 224, 323, 105, 171, 152, 150, 302, 123, 119, 350, 153, 115,\n",
            "        272, 162], device='cuda:0') -> ['0.', '1.', '0.', '1.', '1.', '0.', '1.', '1.']\n",
            "real tensor([0, 1, 1, 1, 0, 0, 0, 1])\n",
            "tensor([365, 172, 224, 323, 105, 171, 152, 150, 302, 123, 119, 350, 153, 115,\n",
            "        272, 162], device='cuda:0') -> ['0.', '1.', '0.', '1.', '1.', '0.', '1.', '1.']\n",
            "real tensor([0, 1, 1, 1, 0, 0, 0, 1])\n",
            "tensor([365, 172, 224, 323, 105, 171, 152, 150, 302, 123, 119, 350, 153, 115,\n",
            "        272, 162], device='cuda:0') -> ['0.', '1.', '0.', '1.', '1.', '0.', '1.', '1.']\n",
            "real tensor([1, 1, 1, 0, 1, 1, 1, 1])\n",
            "tensor([106, 333, 324, 283, 109, 132, 189, 378, 358, 156, 143,  98, 361, 325,\n",
            "        369, 431], device='cuda:0') -> ['1.', '0.', '0.', '1.', '0.', '0.', '0.', '0.']\n",
            "real tensor([1, 1, 1, 0, 1, 1, 1, 1])\n",
            "tensor([106, 333, 324, 283, 109, 132, 189, 378, 358, 156, 143,  98, 361, 325,\n",
            "        369, 431], device='cuda:0') -> ['1.', '0.', '0.', '1.', '0.', '0.', '0.', '0.']\n",
            "real tensor([1, 1, 1, 0, 1, 1, 1, 1])\n",
            "tensor([106, 333, 324, 283, 109, 132, 189, 378, 358, 156, 143,  98, 361, 325,\n",
            "        369, 431], device='cuda:0') -> ['1.', '0.', '0.', '1.', '0.', '0.', '0.', '0.']\n",
            "real tensor([1, 1, 1, 0, 1, 1, 1, 1])\n",
            "tensor([106, 333, 324, 283, 109, 132, 189, 378, 358, 156, 143,  98, 361, 325,\n",
            "        369, 431], device='cuda:0') -> ['1.', '0.', '0.', '1.', '0.', '0.', '0.', '0.']\n",
            "real tensor([1, 1, 1, 0, 1, 1, 1, 1])\n",
            "tensor([106, 333, 324, 283, 109, 132, 189, 378, 358, 156, 143,  98, 361, 325,\n",
            "        369, 431], device='cuda:0') -> ['1.', '0.', '0.', '1.', '0.', '0.', '0.', '0.']\n",
            "real tensor([1, 1, 1, 0, 1, 1, 1, 1])\n",
            "tensor([106, 333, 324, 283, 109, 132, 189, 378, 358, 156, 143,  98, 361, 325,\n",
            "        369, 431], device='cuda:0') -> ['1.', '0.', '0.', '1.', '0.', '0.', '0.', '0.']\n",
            "real tensor([1, 1, 1, 0, 1, 1, 1, 1])\n",
            "tensor([106, 333, 324, 283, 109, 132, 189, 378, 358, 156, 143,  98, 361, 325,\n",
            "        369, 431], device='cuda:0') -> ['1.', '0.', '0.', '1.', '0.', '0.', '0.', '0.']\n",
            "real tensor([0, 0, 1, 0, 0, 0, 1, 0])\n",
            "tensor([360, 304, 347, 352, 141, 390, 179, 360, 283, 310, 289, 250, 100, 371,\n",
            "         85, 361], device='cuda:0') -> ['1.', '1.', '0.', '1.', '0.', '0.', '1.', '0.']\n",
            "real tensor([0, 0, 1, 0, 0, 0, 1, 0])\n",
            "tensor([360, 304, 347, 352, 141, 390, 179, 360, 283, 310, 289, 250, 100, 371,\n",
            "         85, 361], device='cuda:0') -> ['1.', '1.', '0.', '1.', '0.', '0.', '1.', '0.']\n",
            "real tensor([0, 0, 1, 0, 0, 0, 1, 0])\n",
            "tensor([360, 304, 347, 352, 141, 390, 179, 360, 283, 310, 289, 250, 100, 371,\n",
            "         85, 361], device='cuda:0') -> ['1.', '1.', '0.', '1.', '0.', '0.', '1.', '0.']\n",
            "real tensor([0, 0, 1, 0, 0, 0, 1, 0])\n",
            "tensor([360, 304, 347, 352, 141, 390, 179, 360, 283, 310, 289, 250, 100, 371,\n",
            "         85, 361], device='cuda:0') -> ['1.', '1.', '0.', '1.', '0.', '0.', '1.', '0.']\n",
            "real tensor([1, 0, 0, 1, 0, 0, 0, 1])\n",
            "tensor([126, 158, 150, 153, 318, 385, 322, 296, 287, 293, 237, 292, 158, 194,\n",
            "        145, 125], device='cuda:0') -> ['1.', '0.', '1.', '1.', '0.', '0.', '0.', '1.']\n",
            "real tensor([0, 0, 1, 1, 1, 0, 1, 1])\n",
            "tensor([154, 347, 358, 366, 184, 127, 327,  77, 127, 338, 255, 389, 173, 193,\n",
            "        364, 158], device='cuda:0') -> ['1.', '0.', '0.', '1.', '1.', '0.', '1.', '1.']\n",
            "real tensor([0, 0, 1, 1, 1, 0, 1, 1])\n",
            "tensor([154, 347, 358, 366, 184, 127, 327,  77, 127, 338, 255, 389, 173, 193,\n",
            "        364, 158], device='cuda:0') -> ['1.', '0.', '0.', '1.', '1.', '0.', '1.', '1.']\n",
            "real tensor([1, 1, 0, 0, 1, 0, 0, 1])\n",
            "tensor([311, 134, 327, 340,  93, 336, 320, 308, 121, 361, 168,  23, 322,  76,\n",
            "        136, 136], device='cuda:0') -> ['1.', '0.', '1.', '1.', '1.', '0.', '0.', '1.']\n",
            "real tensor([1, 1, 0, 0, 1, 0, 0, 1])\n",
            "tensor([311, 134, 327, 340,  93, 336, 320, 308, 121, 361, 168,  23, 322,  76,\n",
            "        136, 136], device='cuda:0') -> ['1.', '0.', '1.', '1.', '1.', '0.', '0.', '1.']\n",
            "real tensor([1, 1, 0, 0, 1, 0, 0, 1])\n",
            "tensor([311, 134, 327, 340,  93, 336, 320, 308, 121, 361, 168,  23, 322,  76,\n",
            "        136, 136], device='cuda:0') -> ['1.', '0.', '1.', '1.', '1.', '0.', '0.', '1.']\n",
            "real tensor([1, 1, 0, 0, 1, 0, 1, 0])\n",
            "tensor([351, 357, 252, 159, 114, 137, 303, 105,  97, 255, 122, 355, 368, 290,\n",
            "        183, 359], device='cuda:0') -> ['1.', '0.', '0.', '0.', '1.', '0.', '1.', '0.']\n",
            "real tensor([0, 0, 1, 0, 0, 0, 1, 0])\n",
            "tensor([331, 313, 296, 345, 174, 343,  89, 274, 333, 299, 350, 339, 242, 339,\n",
            "        109, 334], device='cuda:0') -> ['0.', '0.', '1.', '0.', '1.', '0.', '1.', '0.']\n",
            "real tensor([1, 0, 0, 1, 1, 1, 1, 0])\n",
            "tensor([180, 165, 121, 335, 319, 314, 325, 159, 363, 358, 369, 145, 126, 243,\n",
            "        140, 389], device='cuda:0') -> ['1.', '1.', '0.', '1.', '0.', '1.', '1.', '0.']\n",
            "real tensor([1, 0, 0, 1, 1, 1, 1, 0])\n",
            "tensor([180, 165, 121, 335, 319, 314, 325, 159, 363, 358, 369, 145, 126, 243,\n",
            "        140, 389], device='cuda:0') -> ['1.', '1.', '0.', '1.', '0.', '1.', '1.', '0.']\n",
            "real tensor([0, 1, 1, 0, 0, 0, 0, 1])\n",
            "tensor([106, 293, 368, 115, 156, 166, 133, 192, 129, 308, 364, 216,  98, 191,\n",
            "        222, 142], device='cuda:0') -> ['0.', '1.', '0.', '0.', '0.', '0.', '1.', '1.']\n",
            "real tensor([0, 1, 1, 0, 0, 0, 0, 1])\n",
            "tensor([106, 293, 368, 115, 156, 166, 133, 192, 129, 308, 364, 216,  98, 191,\n",
            "        222, 142], device='cuda:0') -> ['0.', '1.', '0.', '0.', '0.', '0.', '1.', '1.']\n",
            "real tensor([1, 1, 1, 1, 1, 0, 0, 0])\n",
            "tensor([ 70, 171, 177, 340, 314, 162,  80, 110, 360, 381, 310, 172, 180, 316,\n",
            "        249, 329], device='cuda:0') -> ['1.', '1.', '0.', '1.', '0.', '0.', '1.', '0.']\n",
            "real tensor([1, 1, 1, 1, 1, 0, 0, 0])\n",
            "tensor([ 70, 171, 177, 340, 314, 162,  80, 110, 360, 381, 310, 172, 180, 316,\n",
            "        249, 329], device='cuda:0') -> ['1.', '1.', '0.', '1.', '0.', '0.', '1.', '0.']\n",
            "real tensor([1, 1, 1, 1, 1, 0, 0, 0])\n",
            "tensor([ 70, 171, 177, 340, 314, 162,  80, 110, 360, 381, 310, 172, 180, 316,\n",
            "        249, 329], device='cuda:0') -> ['1.', '1.', '0.', '1.', '0.', '0.', '1.', '0.']\n",
            "real tensor([0, 0, 0, 1, 1, 0, 1, 1])\n",
            "tensor([333, 299, 214, 310, 154, 161, 347, 164, 322, 348,  63, 238, 109, 168,\n",
            "        355, 169], device='cuda:0') -> ['0.', '1.', '1.', '0.', '1.', '0.', '1.', '1.']\n",
            "real tensor([0, 0, 0, 1, 1, 0, 1, 1])\n",
            "tensor([333, 299, 214, 310, 154, 161, 347, 164, 322, 348,  63, 238, 109, 168,\n",
            "        355, 169], device='cuda:0') -> ['0.', '1.', '1.', '0.', '1.', '0.', '1.', '1.']\n",
            "real tensor([0, 0, 0, 1, 1, 0, 1, 1])\n",
            "tensor([333, 299, 214, 310, 154, 161, 347, 164, 322, 348,  63, 238, 109, 168,\n",
            "        355, 169], device='cuda:0') -> ['0.', '1.', '1.', '0.', '1.', '0.', '1.', '1.']\n",
            "real tensor([1, 1, 1, 0, 1, 1, 0, 1])\n",
            "tensor([343, 384, 181, 343, 305, 125, 307, 347, 130, 167, 324, 256, 129, 336,\n",
            "        217, 124], device='cuda:0') -> ['0.', '0.', '0.', '1.', '1.', '1.', '0.', '1.']\n",
            "real tensor([1, 1, 1, 0, 1, 1, 0, 1])\n",
            "tensor([343, 384, 181, 343, 305, 125, 307, 347, 130, 167, 324, 256, 129, 336,\n",
            "        217, 124], device='cuda:0') -> ['0.', '0.', '0.', '1.', '1.', '1.', '0.', '1.']\n",
            "real tensor([1, 1, 1, 0, 1, 1, 0, 1])\n",
            "tensor([343, 384, 181, 343, 305, 125, 307, 347, 130, 167, 324, 256, 129, 336,\n",
            "        217, 124], device='cuda:0') -> ['0.', '0.', '0.', '1.', '1.', '1.', '0.', '1.']\n",
            "real tensor([1, 1, 1, 0, 1, 1, 0, 1])\n",
            "tensor([343, 384, 181, 343, 305, 125, 307, 347, 130, 167, 324, 256, 129, 336,\n",
            "        217, 124], device='cuda:0') -> ['0.', '0.', '0.', '1.', '1.', '1.', '0.', '1.']\n",
            "real tensor([1, 0, 1, 1, 0, 0, 1, 0])\n",
            "tensor([368, 378, 366, 308, 331, 131, 414, 151, 107, 148, 431, 103, 164, 365,\n",
            "        168, 407], device='cuda:0') -> ['1.', '0.', '0.', '1.', '0.', '0.', '1.', '0.']\n",
            "real tensor([1, 1, 1, 0, 1, 0, 1, 0])\n",
            "tensor([202, 330, 194, 113, 153,  94, 356, 151, 368, 162, 317, 237, 317, 289,\n",
            "        124, 338], device='cuda:0') -> ['1.', '0.', '0.', '1.', '1.', '0.', '1.', '0.']\n",
            "real tensor([1, 1, 1, 0, 1, 0, 1, 0])\n",
            "tensor([202, 330, 194, 113, 153,  94, 356, 151, 368, 162, 317, 237, 317, 289,\n",
            "        124, 338], device='cuda:0') -> ['1.', '0.', '0.', '1.', '1.', '0.', '1.', '0.']\n",
            "real tensor([1, 1, 1, 0, 1, 0, 1, 0])\n",
            "tensor([202, 330, 194, 113, 153,  94, 356, 151, 368, 162, 317, 237, 317, 289,\n",
            "        124, 338], device='cuda:0') -> ['1.', '0.', '0.', '1.', '1.', '0.', '1.', '0.']\n",
            "real tensor([0, 0, 0, 1, 0, 0, 0, 1])\n",
            "tensor([287, 349, 358, 328, 140, 207, 154, 123, 330, 328, 318, 210, 143, 167,\n",
            "        111, 138], device='cuda:0') -> ['1.', '1.', '1.', '0.', '0.', '0.', '0.', '1.']\n",
            "real tensor([0, 0, 0, 1, 0, 0, 0, 1])\n",
            "tensor([287, 349, 358, 328, 140, 207, 154, 123, 330, 328, 318, 210, 143, 167,\n",
            "        111, 138], device='cuda:0') -> ['1.', '1.', '1.', '0.', '0.', '0.', '0.', '1.']\n",
            "real tensor([0, 0, 0, 1, 0, 0, 0, 1])\n",
            "tensor([287, 349, 358, 328, 140, 207, 154, 123, 330, 328, 318, 210, 143, 167,\n",
            "        111, 138], device='cuda:0') -> ['1.', '1.', '1.', '0.', '0.', '0.', '0.', '1.']\n",
            "real tensor([0, 0, 0, 1, 0, 0, 0, 1])\n",
            "tensor([287, 349, 358, 328, 140, 207, 154, 123, 330, 328, 318, 210, 143, 167,\n",
            "        111, 138], device='cuda:0') -> ['1.', '1.', '1.', '0.', '0.', '0.', '0.', '1.']\n",
            "real tensor([1, 1, 1, 0, 0, 0, 1, 0])\n",
            "tensor([377, 306, 177, 141, 404, 125, 355, 155,  93, 134, 363, 337,  99, 317,\n",
            "        115, 284], device='cuda:0') -> ['1.', '1.', '1.', '1.', '0.', '0.', '1.', '0.']\n",
            "real tensor([0, 0, 0, 0, 1, 0, 1, 1])\n",
            "tensor([161, 154, 347, 169, 110,  87, 378, 164, 139, 144, 334, 160,  88, 171,\n",
            "        434, 160], device='cuda:0') -> ['0.', '0.', '1.', '0.', '1.', '1.', '1.', '1.']\n",
            "real tensor([0, 0, 0, 0, 1, 0, 1, 1])\n",
            "tensor([161, 154, 347, 169, 110,  87, 378, 164, 139, 144, 334, 160,  88, 171,\n",
            "        434, 160], device='cuda:0') -> ['0.', '0.', '1.', '0.', '1.', '1.', '1.', '1.']\n",
            "real tensor([0, 0, 1, 0, 1, 1, 1, 1])\n",
            "tensor([226, 316, 195, 119, 365, 353, 352, 132, 156, 306, 431, 145, 343, 301,\n",
            "        367, 168], device='cuda:0') -> ['0.', '0.', '0.', '0.', '1.', '1.', '1.', '1.']\n",
            "real tensor([1, 0, 0, 1, 1, 1, 1, 0])\n",
            "tensor([185, 113, 104, 354, 302, 362, 330, 141, 321, 314, 345, 257, 205, 106,\n",
            "        172, 442], device='cuda:0') -> ['0.', '1.', '1.', '0.', '1.', '1.', '1.', '0.']\n",
            "real tensor([1, 0, 0, 1, 1, 1, 1, 0])\n",
            "tensor([185, 113, 104, 354, 302, 362, 330, 141, 321, 314, 345, 257, 205, 106,\n",
            "        172, 442], device='cuda:0') -> ['0.', '1.', '1.', '0.', '1.', '1.', '1.', '0.']\n",
            "real tensor([1, 0, 0, 1, 1, 1, 1, 0])\n",
            "tensor([185, 113, 104, 354, 302, 362, 330, 141, 321, 314, 345, 257, 205, 106,\n",
            "        172, 442], device='cuda:0') -> ['0.', '1.', '1.', '0.', '1.', '1.', '1.', '0.']\n",
            "real tensor([1, 0, 0, 1, 1, 1, 1, 0])\n",
            "tensor([185, 113, 104, 354, 302, 362, 330, 141, 321, 314, 345, 257, 205, 106,\n",
            "        172, 442], device='cuda:0') -> ['0.', '1.', '1.', '0.', '1.', '1.', '1.', '0.']\n",
            "real tensor([1, 1, 0, 1, 1, 1, 1, 1])\n",
            "tensor([112, 208, 369, 142, 117, 156, 196, 329, 332, 336, 120, 365, 371, 305,\n",
            "        230, 173], device='cuda:0') -> ['1.', '1.', '1.', '1.', '0.', '1.', '0.', '1.']\n",
            "real tensor([1, 1, 0, 1, 1, 1, 1, 1])\n",
            "tensor([112, 208, 369, 142, 117, 156, 196, 329, 332, 336, 120, 365, 371, 305,\n",
            "        230, 173], device='cuda:0') -> ['1.', '1.', '1.', '1.', '0.', '1.', '0.', '1.']\n",
            "real tensor([1, 1, 0, 1, 1, 1, 1, 1])\n",
            "tensor([112, 208, 369, 142, 117, 156, 196, 329, 332, 336, 120, 365, 371, 305,\n",
            "        230, 173], device='cuda:0') -> ['1.', '1.', '1.', '1.', '0.', '1.', '0.', '1.']\n",
            "real tensor([1, 1, 1, 0, 0, 0, 0, 0])\n",
            "tensor([135, 350, 332,  95,   8, 163, 105, 179, 338, 147, 156, 353, 378, 286,\n",
            "        292, 368], device='cuda:0') -> ['0.', '1.', '1.', '0.', '0.', '0.', '0.', '0.']\n",
            "real tensor([1, 1, 1, 0, 1, 1, 0, 1])\n",
            "tensor([342, 288, 136, 284, 334,  76, 391, 360, 119, 119, 341, 120, 227, 325,\n",
            "        149, 168], device='cuda:0') -> ['1.', '1.', '1.', '0.', '0.', '1.', '0.', '1.']\n",
            "real tensor([0, 1, 0, 0, 0, 0, 1, 0])\n",
            "tensor([406, 158, 189, 341, 148, 321, 184, 345, 284, 127, 112, 406, 177, 365,\n",
            "        222, 345], device='cuda:0') -> ['0.', '1.', '0.', '0.', '1.', '0.', '0.', '0.']\n",
            "real tensor([0, 1, 0, 0, 0, 0, 1, 0])\n",
            "tensor([406, 158, 189, 341, 148, 321, 184, 345, 284, 127, 112, 406, 177, 365,\n",
            "        222, 345], device='cuda:0') -> ['0.', '1.', '0.', '0.', '1.', '0.', '0.', '0.']\n",
            "real tensor([0, 1, 0, 0, 1, 0, 0, 0])\n",
            "tensor([343,  72, 376, 394,  96, 329, 362, 299, 365, 147, 331, 255,  88, 306,\n",
            "        323, 371], device='cuda:0') -> ['0.', '0.', '1.', '1.', '1.', '0.', '0.', '0.']\n",
            "real tensor([0, 1, 0, 0, 1, 0, 0, 0])\n",
            "tensor([343,  72, 376, 394,  96, 329, 362, 299, 365, 147, 331, 255,  88, 306,\n",
            "        323, 371], device='cuda:0') -> ['0.', '0.', '1.', '1.', '1.', '0.', '0.', '0.']\n",
            "real tensor([0, 1, 0, 0, 1, 0, 0, 0])\n",
            "tensor([343,  72, 376, 394,  96, 329, 362, 299, 365, 147, 331, 255,  88, 306,\n",
            "        323, 371], device='cuda:0') -> ['0.', '0.', '1.', '1.', '1.', '0.', '0.', '0.']\n",
            "real tensor([1, 0, 0, 0, 0, 1, 1, 0])\n",
            "tensor([139, 332, 351, 136, 169, 372, 338, 176, 332, 115, 152, 367, 319, 247,\n",
            "        169, 344], device='cuda:0') -> ['1.', '1.', '0.', '0.', '1.', '0.', '1.', '0.']\n",
            "real tensor([1, 0, 0, 0, 0, 1, 1, 0])\n",
            "tensor([139, 332, 351, 136, 169, 372, 338, 176, 332, 115, 152, 367, 319, 247,\n",
            "        169, 344], device='cuda:0') -> ['1.', '1.', '0.', '0.', '1.', '0.', '1.', '0.']\n",
            "real tensor([1, 0, 0, 0, 0, 1, 1, 0])\n",
            "tensor([139, 332, 351, 136, 169, 372, 338, 176, 332, 115, 152, 367, 319, 247,\n",
            "        169, 344], device='cuda:0') -> ['1.', '1.', '0.', '0.', '1.', '0.', '1.', '0.']\n",
            "real tensor([1, 1, 0, 0, 1, 1, 1, 1])\n",
            "tensor([348, 316,  44, 333, 124, 134, 156, 350, 159, 158, 361,  99, 330, 311,\n",
            "        299, 160], device='cuda:0') -> ['0.', '1.', '0.', '0.', '1.', '1.', '1.', '1.']\n",
            "real tensor([0, 1, 1, 0, 1, 0, 0, 0])\n",
            "tensor([ 88, 128, 159, 325, 121, 329, 282, 300, 127, 149, 147, 243, 160, 358,\n",
            "        336, 356], device='cuda:0') -> ['0.', '0.', '0.', '1.', '1.', '0.', '0.', '0.']\n",
            "real tensor([0, 1, 1, 0, 1, 0, 0, 0])\n",
            "tensor([ 88, 128, 159, 325, 121, 329, 282, 300, 127, 149, 147, 243, 160, 358,\n",
            "        336, 356], device='cuda:0') -> ['0.', '0.', '0.', '1.', '1.', '0.', '0.', '0.']\n",
            "real tensor([0, 1, 1, 0, 1, 0, 0, 0])\n",
            "tensor([ 88, 128, 159, 325, 121, 329, 282, 300, 127, 149, 147, 243, 160, 358,\n",
            "        336, 356], device='cuda:0') -> ['0.', '0.', '0.', '1.', '1.', '0.', '0.', '0.']\n",
            "real tensor([1, 1, 1, 1, 0, 0, 1, 1])\n",
            "tensor([328, 316, 107, 169,  96, 322, 106, 325, 158, 106, 346, 313, 250, 111,\n",
            "        347, 141], device='cuda:0') -> ['1.', '1.', '1.', '1.', '1.', '0.', '1.', '1.']\n",
            "real tensor([1, 1, 1, 1, 0, 0, 0, 0])\n",
            "tensor([342, 123, 121, 327, 159, 158, 122, 110, 130, 356, 238, 227, 357, 326,\n",
            "        313, 345], device='cuda:0') -> ['1.', '1.', '0.', '1.', '0.', '0.', '0.', '0.']\n",
            "real tensor([1, 0, 0, 1, 1, 0, 1, 0])\n",
            "tensor([336, 310, 130, 378, 151, 144, 327,  59, 124, 101, 255, 169, 412, 359,\n",
            "        111, 306], device='cuda:0') -> ['0.', '0.', '1.', '1.', '1.', '0.', '1.', '0.']\n",
            "real tensor([1, 0, 0, 1, 1, 0, 1, 0])\n",
            "tensor([336, 310, 130, 378, 151, 144, 327,  59, 124, 101, 255, 169, 412, 359,\n",
            "        111, 306], device='cuda:0') -> ['0.', '0.', '1.', '1.', '1.', '0.', '1.', '0.']\n",
            "real tensor([1, 0, 0, 0, 0, 0, 0, 1])\n",
            "tensor([283, 325, 369, 289, 366, 312, 336, 369, 115, 166, 124, 232, 117, 145,\n",
            "        177,  79], device='cuda:0') -> ['1.', '0.', '1.', '0.', '0.', '0.', '0.', '1.']\n",
            "real tensor([0, 0, 1, 0, 1, 1, 1, 1])\n",
            "tensor([100, 405,  44, 138, 315, 415, 394,  94, 179, 358, 144, 123, 369, 388,\n",
            "        392, 155], device='cuda:0') -> ['1.', '0.', '1.', '0.', '1.', '1.', '1.', '1.']\n",
            "real tensor([1, 1, 0, 1, 0, 1, 0, 0])\n",
            "tensor([398, 338, 333, 336, 406, 345, 103, 153, 105, 210, 160, 119, 101, 184,\n",
            "        282, 383], device='cuda:0') -> ['1.', '1.', '0.', '1.', '1.', '1.', '0.', '0.']\n",
            "real tensor([1, 1, 1, 0, 1, 1, 0, 0])\n",
            "tensor([136, 128, 320, 137, 121, 318, 158, 172, 358, 321, 135, 374, 343, 227,\n",
            "        249, 308], device='cuda:0') -> ['1.', '1.', '0.', '0.', '0.', '1.', '1.', '0.']\n",
            "real tensor([1, 1, 1, 0, 1, 1, 0, 0])\n",
            "tensor([136, 128, 320, 137, 121, 318, 158, 172, 358, 321, 135, 374, 343, 227,\n",
            "        249, 308], device='cuda:0') -> ['1.', '1.', '0.', '0.', '0.', '1.', '1.', '0.']\n",
            "real tensor([1, 1, 1, 0, 1, 1, 0, 0])\n",
            "tensor([136, 128, 320, 137, 121, 318, 158, 172, 358, 321, 135, 374, 343, 227,\n",
            "        249, 308], device='cuda:0') -> ['1.', '1.', '0.', '0.', '0.', '1.', '1.', '0.']\n",
            "real tensor([0, 1, 0, 1, 1, 1, 1, 1])\n",
            "tensor([355, 363,  89, 298, 341, 331, 361,  49, 337, 342, 246, 343, 316, 361,\n",
            "        326, 110], device='cuda:0') -> ['0.', '1.', '1.', '1.', '1.', '1.', '1.', '1.']\n",
            "real tensor([0, 1, 0, 1, 0, 1, 0, 0])\n",
            "tensor([147, 151, 146, 185, 101, 117, 356, 348, 187, 161, 244, 183, 110, 141,\n",
            "        317, 345], device='cuda:0') -> ['0.', '1.', '1.', '1.', '0.', '1.', '0.', '0.']\n",
            "real tensor([1, 0, 0, 1, 1, 0, 1, 1])\n",
            "tensor([170, 122, 343, 143, 353, 345,  84, 357, 317, 360, 184, 361, 148, 158,\n",
            "        240, 160], device='cuda:0') -> ['1.', '0.', '1.', '1.', '0.', '0.', '0.', '1.']\n",
            "real tensor([1, 0, 0, 1, 1, 0, 1, 1])\n",
            "tensor([170, 122, 343, 143, 353, 345,  84, 357, 317, 360, 184, 361, 148, 158,\n",
            "        240, 160], device='cuda:0') -> ['1.', '0.', '1.', '1.', '0.', '0.', '0.', '1.']\n",
            "real tensor([1, 0, 0, 1, 1, 0, 1, 1])\n",
            "tensor([170, 122, 343, 143, 353, 345,  84, 357, 317, 360, 184, 361, 148, 158,\n",
            "        240, 160], device='cuda:0') -> ['1.', '0.', '1.', '1.', '0.', '0.', '0.', '1.']\n",
            "real tensor([1, 1, 0, 1, 0, 1, 1, 1])\n",
            "tensor([313, 138, 279, 137, 317, 110, 177, 356, 135, 328, 431, 349, 199, 288,\n",
            "        300, 157], device='cuda:0') -> ['0.', '1.', '1.', '1.', '0.', '1.', '1.', '1.']\n",
            "real tensor([1, 1, 0, 1, 0, 1, 1, 1])\n",
            "tensor([313, 138, 279, 137, 317, 110, 177, 356, 135, 328, 431, 349, 199, 288,\n",
            "        300, 157], device='cuda:0') -> ['0.', '1.', '1.', '1.', '0.', '1.', '1.', '1.']\n",
            "real tensor([0, 1, 0, 1, 0, 1, 0, 0])\n",
            "tensor([143, 135, 132, 134, 123, 152, 332, 323, 182, 134, 168, 112, 111, 129,\n",
            "        357, 250], device='cuda:0') -> ['0.', '0.', '1.', '0.', '1.', '0.', '1.', '1.']\n",
            "real tensor([0, 1, 0, 1, 0, 1, 0, 0])\n",
            "tensor([143, 135, 132, 134, 123, 152, 332, 323, 182, 134, 168, 112, 111, 129,\n",
            "        357, 250], device='cuda:0') -> ['0.', '0.', '1.', '0.', '1.', '0.', '1.', '1.']\n",
            "real tensor([0, 1, 0, 1, 0, 1, 0, 0])\n",
            "tensor([143, 135, 132, 134, 123, 152, 332, 323, 182, 134, 168, 112, 111, 129,\n",
            "        357, 250], device='cuda:0') -> ['0.', '0.', '1.', '0.', '1.', '0.', '1.', '1.']\n",
            "real tensor([0, 1, 0, 1, 0, 1, 0, 0])\n",
            "tensor([143, 135, 132, 134, 123, 152, 332, 323, 182, 134, 168, 112, 111, 129,\n",
            "        357, 250], device='cuda:0') -> ['0.', '0.', '1.', '0.', '1.', '0.', '1.', '1.']\n",
            "real tensor([0, 1, 0, 1, 0, 1, 0, 0])\n",
            "tensor([143, 135, 132, 134, 123, 152, 332, 323, 182, 134, 168, 112, 111, 129,\n",
            "        357, 250], device='cuda:0') -> ['0.', '0.', '1.', '0.', '1.', '0.', '1.', '1.']\n",
            "real tensor([0, 1, 0, 1, 0, 1, 0, 0])\n",
            "tensor([143, 135, 132, 134, 123, 152, 332, 323, 182, 134, 168, 112, 111, 129,\n",
            "        357, 250], device='cuda:0') -> ['0.', '0.', '1.', '0.', '1.', '0.', '1.', '1.']\n",
            "real tensor([0, 1, 0, 1, 0, 1, 0, 0])\n",
            "tensor([143, 135, 132, 134, 123, 152, 332, 323, 182, 134, 168, 112, 111, 129,\n",
            "        357, 250], device='cuda:0') -> ['0.', '0.', '1.', '0.', '1.', '0.', '1.', '1.']\n",
            "real tensor([0, 0, 0, 0, 1, 1, 1, 0])\n",
            "tensor([ 91, 135, 133, 330, 132, 152, 113, 354, 188, 188, 181, 350, 108, 121,\n",
            "        262, 328], device='cuda:0') -> ['0.', '0.', '1.', '0.', '0.', '1.', '0.', '0.']\n",
            "real tensor([0, 0, 0, 0, 1, 1, 1, 0])\n",
            "tensor([ 91, 135, 133, 330, 132, 152, 113, 354, 188, 188, 181, 350, 108, 121,\n",
            "        262, 328], device='cuda:0') -> ['0.', '0.', '1.', '0.', '0.', '1.', '0.', '0.']\n",
            "real tensor([0, 0, 0, 0, 1, 1, 1, 0])\n",
            "tensor([ 91, 135, 133, 330, 132, 152, 113, 354, 188, 188, 181, 350, 108, 121,\n",
            "        262, 328], device='cuda:0') -> ['0.', '0.', '1.', '0.', '0.', '1.', '0.', '0.']\n",
            "real tensor([0, 1, 0, 1, 1, 0, 1, 0])\n",
            "tensor([337, 296, 389, 127, 342, 331, 156, 369, 309, 344, 354, 139, 236, 255,\n",
            "        151, 370], device='cuda:0') -> ['1.', '1.', '0.', '1.', '0.', '0.', '1.', '0.']\n",
            "real tensor([0, 1, 0, 1, 1, 0, 1, 0])\n",
            "tensor([337, 296, 389, 127, 342, 331, 156, 369, 309, 344, 354, 139, 236, 255,\n",
            "        151, 370], device='cuda:0') -> ['1.', '1.', '0.', '1.', '0.', '0.', '1.', '0.']\n",
            "real tensor([1, 0, 1, 0, 1, 0, 0, 1])\n",
            "tensor([363, 382, 152, 308, 112, 343, 297, 361, 145, 174, 325, 128, 332, 148,\n",
            "        215, 175], device='cuda:0') -> ['1.', '0.', '1.', '0.', '0.', '0.', '1.', '1.']\n",
            "real tensor([1, 0, 1, 0, 1, 0, 0, 1])\n",
            "tensor([363, 382, 152, 308, 112, 343, 297, 361, 145, 174, 325, 128, 332, 148,\n",
            "        215, 175], device='cuda:0') -> ['1.', '0.', '1.', '0.', '0.', '0.', '1.', '1.']\n",
            "real tensor([0, 0, 0, 1, 0, 1, 1, 1])\n",
            "tensor([316, 189, 101, 311, 140, 328, 323, 136, 362, 114, 239, 356, 121, 319,\n",
            "        340,  80], device='cuda:0') -> ['0.', '0.', '1.', '1.', '0.', '1.', '1.', '1.']\n",
            "real tensor([1, 1, 0, 0, 0, 0, 0, 1])\n",
            "tensor([105, 122, 298, 373, 296, 362, 362, 366, 347, 328, 170, 181,  88, 176,\n",
            "         44,  94], device='cuda:0') -> ['1.', '1.', '1.', '0.', '1.', '0.', '1.', '1.']\n",
            "real tensor([1, 1, 0, 0, 0, 0, 0, 1])\n",
            "tensor([105, 122, 298, 373, 296, 362, 362, 366, 347, 328, 170, 181,  88, 176,\n",
            "         44,  94], device='cuda:0') -> ['1.', '1.', '1.', '0.', '1.', '0.', '1.', '1.']\n",
            "real tensor([1, 1, 0, 0, 0, 0, 0, 1])\n",
            "tensor([105, 122, 298, 373, 296, 362, 362, 366, 347, 328, 170, 181,  88, 176,\n",
            "         44,  94], device='cuda:0') -> ['1.', '1.', '1.', '0.', '1.', '0.', '1.', '1.']\n",
            "real tensor([0, 0, 1, 1, 1, 1, 0, 1])\n",
            "tensor([141,  96, 135, 378, 162, 376, 121, 120, 164, 241, 154, 357, 134, 417,\n",
            "        208, 135], device='cuda:0') -> ['0.', '1.', '1.', '1.', '1.', '1.', '0.', '1.']\n",
            "real tensor([0, 1, 1, 0, 1, 0, 0, 1])\n",
            "tensor([367, 404, 314, 136, 310, 126, 150, 189, 298, 387, 323, 139, 349, 194,\n",
            "         29,  83], device='cuda:0') -> ['0.', '0.', '0.', '0.', '0.', '0.', '1.', '1.']\n",
            "real tensor([0, 1, 1, 0, 1, 0, 0, 1])\n",
            "tensor([367, 404, 314, 136, 310, 126, 150, 189, 298, 387, 323, 139, 349, 194,\n",
            "         29,  83], device='cuda:0') -> ['0.', '0.', '0.', '0.', '0.', '0.', '1.', '1.']\n",
            "real tensor([0, 1, 1, 0, 1, 0, 0, 1])\n",
            "tensor([367, 404, 314, 136, 310, 126, 150, 189, 298, 387, 323, 139, 349, 194,\n",
            "         29,  83], device='cuda:0') -> ['0.', '0.', '0.', '0.', '0.', '0.', '1.', '1.']\n",
            "real tensor([0, 1, 1, 0, 1, 0, 0, 1])\n",
            "tensor([367, 404, 314, 136, 310, 126, 150, 189, 298, 387, 323, 139, 349, 194,\n",
            "         29,  83], device='cuda:0') -> ['0.', '0.', '0.', '0.', '0.', '0.', '1.', '1.']\n",
            "real tensor([0, 1, 0, 1, 0, 1, 1, 0])\n",
            "tensor([361, 169, 431, 115, 312, 145, 119, 349, 338, 102, 406, 124, 369, 182,\n",
            "        135, 321], device='cuda:0') -> ['1.', '1.', '0.', '1.', '0.', '1.', '1.', '0.']\n",
            "real tensor([1, 0, 0, 1, 0, 0, 0, 0])\n",
            "tensor([362, 278, 322, 366, 175, 116, 164, 138, 132, 154, 184, 118, 341, 338,\n",
            "        372, 238], device='cuda:0') -> ['1.', '1.', '1.', '0.', '1.', '1.', '1.', '1.']\n",
            "real tensor([1, 0, 0, 1, 0, 0, 0, 0])\n",
            "tensor([362, 278, 322, 366, 175, 116, 164, 138, 132, 154, 184, 118, 341, 338,\n",
            "        372, 238], device='cuda:0') -> ['1.', '1.', '1.', '0.', '1.', '1.', '1.', '1.']\n",
            "real tensor([1, 0, 0, 1, 0, 0, 0, 0])\n",
            "tensor([362, 278, 322, 366, 175, 116, 164, 138, 132, 154, 184, 118, 341, 338,\n",
            "        372, 238], device='cuda:0') -> ['1.', '1.', '1.', '0.', '1.', '1.', '1.', '1.']\n",
            "real tensor([1, 0, 0, 1, 0, 0, 0, 0])\n",
            "tensor([362, 278, 322, 366, 175, 116, 164, 138, 132, 154, 184, 118, 341, 338,\n",
            "        372, 238], device='cuda:0') -> ['1.', '1.', '1.', '0.', '1.', '1.', '1.', '1.']\n",
            "real tensor([1, 0, 0, 1, 0, 0, 0, 0])\n",
            "tensor([362, 278, 322, 366, 175, 116, 164, 138, 132, 154, 184, 118, 341, 338,\n",
            "        372, 238], device='cuda:0') -> ['1.', '1.', '1.', '0.', '1.', '1.', '1.', '1.']\n",
            "real tensor([1, 0, 0, 1, 0, 0, 0, 0])\n",
            "tensor([362, 278, 322, 366, 175, 116, 164, 138, 132, 154, 184, 118, 341, 338,\n",
            "        372, 238], device='cuda:0') -> ['1.', '1.', '1.', '0.', '1.', '1.', '1.', '1.']\n",
            "real tensor([1, 0, 0, 1, 0, 0, 0, 0])\n",
            "tensor([362, 278, 322, 366, 175, 116, 164, 138, 132, 154, 184, 118, 341, 338,\n",
            "        372, 238], device='cuda:0') -> ['1.', '1.', '1.', '0.', '1.', '1.', '1.', '1.']\n",
            "real tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
            "tensor([332, 176, 129, 128, 204, 191, 176, 314, 124, 275, 331, 331, 340, 339,\n",
            "        218, 161], device='cuda:0') -> ['1.', '1.', '0.', '1.', '0.', '1.', '0.', '1.']\n",
            "real tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
            "tensor([332, 176, 129, 128, 204, 191, 176, 314, 124, 275, 331, 331, 340, 339,\n",
            "        218, 161], device='cuda:0') -> ['1.', '1.', '0.', '1.', '0.', '1.', '0.', '1.']\n",
            "real tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
            "tensor([332, 176, 129, 128, 204, 191, 176, 314, 124, 275, 331, 331, 340, 339,\n",
            "        218, 161], device='cuda:0') -> ['1.', '1.', '0.', '1.', '0.', '1.', '0.', '1.']\n",
            "real tensor([1, 0, 0, 0, 0, 1, 0, 0])\n",
            "tensor([368, 304, 151, 121, 310, 342,  85, 141, 166, 106, 255, 360, 115, 119,\n",
            "        348, 322], device='cuda:0') -> ['0.', '0.', '1.', '0.', '0.', '1.', '0.', '0.']\n",
            "real tensor([1, 0, 0, 0, 0, 1, 0, 0])\n",
            "tensor([368, 304, 151, 121, 310, 342,  85, 141, 166, 106, 255, 360, 115, 119,\n",
            "        348, 322], device='cuda:0') -> ['0.', '0.', '1.', '0.', '0.', '1.', '0.', '0.']\n",
            "real tensor([0, 0, 0, 1, 1, 0, 1, 0])\n",
            "tensor([148, 161, 403, 135, 368, 330, 161, 388,  99,  16, 355, 116, 354, 337,\n",
            "        158, 329], device='cuda:0') -> ['0.', '1.', '0.', '1.', '1.', '0.', '1.', '0.']\n",
            "real tensor([1, 1, 1, 0, 0, 1, 1, 1])\n",
            "tensor([353, 352, 329, 317, 395, 168, 108, 373, 108, 117, 124, 152,  96, 245,\n",
            "        326, 169], device='cuda:0') -> ['1.', '1.', '1.', '0.', '0.', '0.', '1.', '1.']\n",
            "real tensor([1, 0, 1, 0, 1, 0, 0, 0])\n",
            "tensor([157, 129, 409, 147, 370, 143, 115, 150, 270, 351, 241, 327, 140, 328,\n",
            "        328, 379], device='cuda:0') -> ['1.', '0.', '0.', '0.', '1.', '0.', '0.', '0.']\n",
            "real tensor([1, 0, 1, 1, 1, 1, 0, 0])\n",
            "tensor([121, 178, 162, 329, 134, 390,  65, 165, 352, 346, 358, 102, 389,  84,\n",
            "        334, 330], device='cuda:0') -> ['1.', '0.', '1.', '1.', '1.', '1.', '1.', '0.']\n",
            "real tensor([1, 0, 1, 0, 0, 1, 1, 0])\n",
            "tensor([378, 381, 126, 156, 105, 304, 341, 150, 158, 197, 346, 413, 333, 144,\n",
            "        247, 410], device='cuda:0') -> ['1.', '0.', '0.', '0.', '1.', '1.', '0.', '0.']\n",
            "real tensor([1, 0, 1, 0, 0, 1, 1, 0])\n",
            "tensor([378, 381, 126, 156, 105, 304, 341, 150, 158, 197, 346, 413, 333, 144,\n",
            "        247, 410], device='cuda:0') -> ['1.', '0.', '0.', '0.', '1.', '1.', '0.', '0.']\n",
            "real tensor([1, 0, 1, 0, 0, 1, 1, 0])\n",
            "tensor([378, 381, 126, 156, 105, 304, 341, 150, 158, 197, 346, 413, 333, 144,\n",
            "        247, 410], device='cuda:0') -> ['1.', '0.', '0.', '0.', '1.', '1.', '0.', '0.']\n",
            "正确率 4.4515380859375\n",
            "141\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMsPb5CY3KB6",
        "outputId": "8068c6d5-0257-4bd3-b41d-190cef618c9f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "uKGm2r2ioSx9"
      }
    }
  ]
}